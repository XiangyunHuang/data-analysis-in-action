# 时间序列回归 {#sec-time-series-regression}

```{r}
#| echo: false

source("_common.R")
```

```{r}
#| message: false

library(cmdstanr)
library(zoo)
library(xts) # xts 依赖 zoo
library(fGarch)
library(INLA)
library(mgcv)
library(tensorflow)
library(ggplot2)
library(bayesplot)
```

## 随机波动率模型

随机波动率模型主要用于股票时间序列数据建模。本节以美团股价数据为例介绍随机波动率模型，并分别以 Stan 框架和 **fGarch** 包拟合模型。

```{r}
#| label: fig-meituan-stack
#| message: false
#| fig-cap: 美团股价走势
#| fig-showtext: true
#| fig-width: 5
#| fig-height: 4

# 美团上市至 2023-07-15
meituan <- readRDS(file = "data/meituan.rds")
library(zoo)
library(xts)
library(ggplot2)
autoplot(meituan[, "3690.HK.Adjusted"]) +
  theme_classic() +
  labs(x = "日期", y = "股价")
```

对数收益率的计算公式如下：

$$
\mathrm{对数收益率} = \ln(\mathrm{今日收盘价} / \mathrm{上一收盘价} ) = \ln (1 + \mathrm{普通收益率})
$$

下图给出股价对数收益率变化和股价对数收益率的分布，可以看出在不同时间段，收益率波动幅度是不同的，美团股价对数收益率的分布可以看作正态分布。

```{r}
#| label: fig-meituan-log-return
#| fig-cap: 美团股价对数收益率的情况
#| fig-subcap: 
#|  - 对数收益率的变动
#|  - 对数收益率的分布
#| fig-showtext: true
#| fig-width: 5
#| fig-height: 4
#| layout-ncol: 2

meituan_log_return <- log(1 + diff(meituan[, "3690.HK.Adjusted"]) /  meituan[, "3690.HK.Adjusted"])[-1,]
autoplot(meituan_log_return) +
  theme_classic() +
  labs(x = "日期", y = "对数收益率")
ggplot(data = meituan_log_return, aes(x = `3690.HK.Adjusted`)) +
  geom_histogram(color = "black", fill = "gray", bins = 30) +
  theme_classic() +
  labs(x = "对数收益率", y = "频数（天数）")
```

检查对数收益率序列的自相关图

```{r}
#| label: fig-log-return
#| fig-cap: 对数收益率的自相关图
#| fig-showtext: true
#| fig-width: 5
#| fig-height: 4
#| par: true

acf(meituan_log_return, main = "")
```

发现，滞后 2、3、6、26 阶都有出界，滞后 17 阶略微出界，其它的自相关都在零水平线的界限内。

```{r}
Box.test(meituan_log_return, lag = 12, type = "Ljung")
```

在 0.05 水平下拒绝了白噪声检验，说明对数收益率序列存在相关性。同理，也注意到对数收益率的绝对值和平方序列都不是独立的，存在相关性。

```{r}
# ARCH 效应的检验
Box.test((meituan_log_return - mean(meituan_log_return))^2, lag = 12, type = "Ljung")
```

结果高度显著，说明有 ARCH 效应。

### Stan 框架

```{r}
library(cmdstanr)
```

### fGarch 包

[《金融时间序列分析讲义》](https://www.math.pku.edu.cn/teachers/lidf/course/fts/ftsnotes/html/_ftsnotes/index.html)两个波动率建模方法

-   自回归条件异方差模型（Autoregressive Conditional Heteroskedasticity，简称 ARCH）。
-   广义自回归条件异方差模型 （Generalized Autoregressive Conditional Heteroskedasticity，简称 GARCH ）

确定 ARCH 模型的阶，观察残差的平方的 ACF 和 PACF 。

```{r}
#| label: fig-log-return-resid
#| fig-cap: 对数收益率的残差平方
#| fig-subcap: 
#| - 自相关图
#| - 偏自相关图
#| fig-showtext: true
#| fig-width: 5
#| fig-height: 4
#| layout-ncol: 2
#| par: true

acf((meituan_log_return - mean(meituan_log_return))^2, main = "")
pacf((meituan_log_return - mean(meituan_log_return))^2, main = "")
```

发现 ACF 在滞后 1、2、3 阶比较突出，PACF 在滞后 1、2、16、18、29 阶比较突出。所以下面先来考虑低阶的 ARCH(2) 模型，设 $r_t$ 为对数收益率。

$$
\begin{aligned}
r_t &= \mu + a_t, \quad a_t = \sigma_t \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0,1) \\
\sigma_t^2 &= \alpha_0 + \alpha_1 a_{t-1}^2 
  + \alpha_2 a_{t-2}^2.
\end{aligned}
$$

拟合 ARCH 模型，比较模型估计结果，根据系数显著性的结果，采纳 ARCH(2) 模型。

```{r}
#| message: false

library(fGarch)
meituan_garch1 <- garchFit(~ 1 + garch(2, 0), data = meituan_log_return, trace = FALSE, cond.dist = "std")
summary(meituan_garch1)
```

函数 `garchFit()` 的参数 `cond.dist` 默认值为 `"norm"` 表示标准正态分布，`cond.dist = "std"` 表示标准 t 分布。模型均值的估计值接近 0 是符合预期的，且显著性没通过，对数收益率在 0 上下波动。将估计结果代入模型，得到

$$
\begin{aligned}
r_t &= -5.665 \times 10^{-5} + a_t, \quad a_t = \sigma_t \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0,1) \\
\sigma_t^2 &= 1.070 \times 10^{-3} + 0.1156 a_{t-1}^2 + 0.1438a_{t-2}^2.
\end{aligned}
$$

下面考虑 GARCH(1,1) 模型

$$
\begin{aligned}
r_t &= \mu + a_t, \quad a_t = \sigma_t \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0,1) \\
\sigma_t^2 &= \alpha_0 + \alpha_1 a_{t-1}^2 
  + \beta_1 \sigma_{t-1}^2.
\end{aligned}
$$

```{r}
meituan_garch2 <- garchFit(~ 1 + garch(1, 1), data = meituan_log_return, trace = FALSE, cond.dist = "std")
summary(meituan_garch2)
```

波动率的贡献主要来自 $\sigma_{t-1}^2$ ，其系数 $\beta_1$ 为 0.918。通过对数似然的比较，可以发现 GARCH(1,1) 模型比 ARCH(2) 模型更好。

## 贝叶斯可加模型

大规模时间序列回归，观察值是比较多的，可达数十万、数百万，乃至更多。粗粒度时时间跨度往往很长，比如数十年的天粒度数据，细粒度时时间跨度可短可长，比如数年的半小时级数据，总之，需要包含多个季节的数据，各种季节性重复出现。通过时序图可以观察到明显的季节性，而且往往是多种周期不同的季节性混合在一起，有时还包含一定的趋势性。举例来说，比如 2018-2023 年美国旧金山犯罪[事件报告数据](https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783)，事件数量的变化趋势，除了上述季节性因素，特殊事件疫情肯定会影响，数据规模约 200 M 。再比如 2018-2023 年美国境内和跨境旅游业中的[航班数据](https://www.transtats.bts.gov/)，原始数据非常大，R 包 [nycflights13](https://github.com/tidyverse/nycflights13) 提供纽约机场的部分航班数据。

### Stan 框架

```{r}
library(cmdstanr)
```

### INLA 框架 {#sec-kaust-inla}

模型内容、成分结构和参数解释

阿卜杜拉国王科技大学（King Abdullah University of Science and Technology 简称 KAUST）的 Håvard Rue 等开发了 INLA 框架 [@Rue2009]。INLA 动态时间序列建模 [@Nalini2022]

```{r}
#| message: false

library(INLA)
```

## 一些非参数模型

### mgcv 包 {#sec-gnu-mgcv}

模型内容、成分结构和参数解释。一般可加模型，在似然函数中添加平滑样条，与 Lasso 回归模型在形式上有相似之处，属于频率派方法。

**mgcv** 包 [@Wood2017] 是 R 软件内置的推荐组件，由 Simon Wood 开发和维护，历经多年，成熟稳定。对于时间序列数据预测，数万和百万级观测值都可以 [@wood2015]。函数 `bam()`

```{r}
library(mgcv)
```

### tensorflow 框架 {#sec-google-tensorflow}

前面介绍的模型都具有非常强的可解释性，比如各个参数对模型的作用。对于复杂的时间序列数据，比较适合用复杂的模型来拟合，看重模型的泛化能力，而不那么关注模型的机理。

多层感知机是一种全连接层的前馈神经网络。**nnet** 包的函数 `nnet()` 实现了单隐藏层的简单前馈神经网络，可用于时间序列预测，也可用于分类数据的预测。作为对比的基础，下面先用 nnet 包训练和预测数据。

```{r}
# 准备数据
air_passengers <- as.matrix(embed(AirPassengers, 4))
colnames(air_passengers) <- c("y", "x3", "x2", "x1")
data_size <- nrow(air_passengers)
# 拆分数据集
train_size <- floor(data_size * 0.67)
train_data <- air_passengers[1:train_size, ]
test_data <- air_passengers[train_size:data_size, ]

# 随机数种子对结果的影响非常大 试试 set.seed(20232023) 
set.seed(20222022) 
# 单隐藏层 8 个神经元
mod_nnet <- nnet::nnet(
  y ~ x1 + x2 + x3,
  data = air_passengers, # 数据集
  subset = 1:train_size, # 训练数据的指标向量
  linout = TRUE, size = 4, rang = 0.1,
  decay = 5e-4, maxit = 400, trace = FALSE
)
# 预测
train_pred <- predict(mod_nnet, newdata = air_passengers[1:train_size,], type = "raw")
# 训练集 RMSE
sqrt(mean((air_passengers[1:train_size, "y"] - train_pred )^2))
# 预测
test_pred <- predict(mod_nnet, newdata = air_passengers[-(1:train_size),], type = "raw")
# 测试集 RMSE
sqrt(mean((air_passengers[-(1:train_size), "y"] - test_pred)^2))
```

下面将原观测序列，训练集和测试集上的预测序列放在一张图上展示。图中，红色曲线表示训练集上的预测结果，绿色曲线为测试集上预测结果。

```{r}
#| label: fig-nnet
#| fig-cap: 单层感知机预测
#| fig-showtext: true
#| fig-width: 5
#| fig-height: 4
#| par: true

train_pred_ts <- ts(data = train_pred, start = c(1949, 3), frequency = 12)
test_pred_ts <- ts(data = test_pred, start = c(1957, 1), frequency = 12)
plot(AirPassengers)
lines(train_pred_ts, col = "red")
lines(test_pred_ts, col = "green")
```

由图可知，在测试集上，随着时间拉长，预测越来越不准。

下面使用 tensorflow 包构造**多层**感知机训练数据和预测。

```{r}
#| message: false

library(tensorflow)
library(keras)
set_random_seed(20222022)
# 模型结构
mod_mlp <- keras_model_sequential() |> 
  layer_dense(units = 12, activation = "relu", input_shape = c(3)) |> 
  layer_dense(units = 8, activation = "relu") |> 
  layer_dense(units = 1)
# 训练目标
compile(mod_mlp,
  loss = "mse", # 损失函数
  optimizer = "adam", # 优化器
  metrics = "mae" # 监控度量
)
# 模型概览
summary(mod_mlp)
```

输入层为 3 个节点，中间两个隐藏层，第一层为 12 个节点，第二层为 8 个节点，全连接网络，最后输出为一层单节点，意味着单个输出。每一层都有节点和权重，参数总数为 161。

```{r}
# 拟合模型
fit(mod_mlp,
  x = train_data[, c("x1", "x2", "x3")],
  y = train_data[, "y"],
  epochs = 200,
  batch_size = 10, # 每次更新梯度所用的样本量
  validation_split = 0.2, # 从训练数据中拆分一部分用作验证集
  verbose = 0 # 不显示训练进度
)
# 将测试数据代入模型，计算损失函数和监控度量
evaluate(mod_mlp, test_data[, c("x1", "x2", "x3")], test_data[, "y"])
# 测试集上的预测
mlp_test_pred <- predict(mod_mlp, test_data[, c("x1", "x2", "x3")]) 
mlp_train_pred <- predict(mod_mlp, train_data[, c("x1", "x2", "x3")]) 
sqrt(mean((test_data[, "y"] - mlp_test_pred)^2)) # 计算均方根误差
```

从 RMSE 来看，MLP（多层感知机）预测效果比单层感知机稍好些，可网络复杂度是增加很多的。

```{r}
#| label: fig-tensorflow-mlp
#| fig-cap: 多层感知机预测
#| fig-showtext: true
#| fig-width: 5
#| fig-height: 4
#| par: true

mlp_train_pred_ts <- ts(data = mlp_train_pred, start = c(1949, 3), frequency = 12)
mlp_test_pred_ts <- ts(data = mlp_test_pred, start = c(1957, 1), frequency = 12)
plot(AirPassengers)
lines(mlp_train_pred_ts, col = "red")
lines(mlp_test_pred_ts, col = "green")
```

下面用 LSTM （长短期记忆）神经网络来训练时间序列数据，预测未来一周的趋势。输出不再是一天（单点输出），而是 7 天的预测值（多点输出）。参考 **tensorflow** 包的[官网](https://tensorflow.rstudio.com/guides/keras/working_with_rnns#introduction)中 RNN 递归神经网络的介绍。

## 习题

1.  基于 R 软件内置的数据集 `sunspots` 和 `sunspot.month` 比较 INLA 和 **mgcv** 框架的预测效果。

    ```{r}
    #| label: fig-sunspots
    #| fig-cap: 预测月粒度太阳黑子数量
    #| fig-width: 7
    #| fig-height: 4
    #| fig-showtext: true
    #| code-fold: true
    #| echo: !expr knitr::is_html_output()

    sunspots_tbl <- broom::tidy(sunspots)
    sunspots_month_tbl <- broom::tidy(sunspot.month)
    ggplot() +
      geom_line(data = sunspots_month_tbl, aes(x = index, y = value), color = "red") +
      geom_line(data = sunspots_tbl, aes(x = index, y = value)) +
      theme_bw() +
      labs(x = "年月", y = "数量")
    ```

    图中黑线和红线分别表示 1749-1983 年、1984-2014 年每月太阳黑子数量。
