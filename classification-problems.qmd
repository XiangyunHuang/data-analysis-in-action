# 分类问题 {#sec-classification-problems}

```{r}
#| message: false

library(nnet)     # 多项回归/神经网络 multinom / nnet
library(MASS)     # 线性/二次判别分析 lda / qda
library(glmnet)   # 惩罚多项回归 glmnet
library(e1071)    # 朴素贝叶斯 naiveBayes 和支持向量机 svm
library(kernlab)  # 支持向量机分类 ksvm
library(class)    # K 最近邻 knn
library(rpart)    # 决策树分类 rpart
library(randomForest) # 随机森林 randomForest
# library(gbm)        # 梯度提升机
library(xgboost)      # 集成学习
library(lattice)
```

以 iris 数据集为例，简单，方便介绍模型和算法，定位入门。分类间隔最大化，也是一个优化问题，找一条分界线，一个分割面，一个超平面划分不同的种类。本章篇幅：每个算法 4 页，共计 40 页。10 个算法的介绍按照分类思路，模型，代码和参数说明，分类性能评估。应用案例是手写数字识别。要点不是数据如何复杂，而是怎样把理论写得通俗、准确，看了之后能够应用到复杂的真实数据分析场景中去。理论解释、绘图说明、经验总结。

-   线性分类器
    -   多项回归模型
    -   线性判别分析
-   非线性分类器
    -   二次判别分析
    -   朴素贝叶斯
    -   支持向量机
    -   K 最近邻
    -   神经网络
    -   决策树
    -   随机森林
    -   集成学习

## 多项回归模型 {#sec-multinomial-regression-models}

```{r}
library(nnet) # 多项逻辑回归
iris_multinom <- multinom(Species ~ ., data = iris, trace = FALSE)
summary(iris_multinom)
```

```{r}
table(predict(iris_multinom, iris[, -5], type = "class"), iris[, 5])
```

在有的数据中，观测变量之间存在共线性，采用变量选择方法，比如 Lasso 方法压缩掉一部分变量。

```{r}
library(glmnet) # 多项回归
iris_glmnet <- glmnet(x = iris[, -5], y = iris[, 5], family = "multinomial")
```

```{r}
#| label: fig-multinom-glmnet
#| fig-cap: "回归系数的迭代路径"
#| fig-width: 5
#| fig-height: 5
#| fig-showtext: true

plot(iris_glmnet)
```

```{r}
#| label: fig-multinom-lambda
#| fig-cap: "惩罚系数的迭代路径"
#| fig-width: 5
#| fig-height: 5
#| fig-showtext: true

plot(iris_glmnet$lambda,
  ylab = expression(lambda), xlab = "迭代次数",
  main = "惩罚系数的迭代路径"
)
```

选择一个迭代趋于稳定时的 lambda，比如 `iris_glmnet$lambda[80]` 。

```{r}
coef(iris_glmnet, s = 0.0002796185)
```

```{r}
iris_pred_glmnet <- predict(
  object = iris_glmnet, newx = as.matrix(iris[, -5]),
  s = 0.0002796185, type = "class"
)
```

```{r}
table(iris_pred_glmnet, iris[, 5])
```

## 线性判别分析 {#sec-linear-discriminant-analysis}

```{r}
library(MASS)
# lda
iris_lda <- lda(Species ~ ., data=iris)
iris_lda
```

```{r}
table(predict(iris_lda, iris[, -5])$class, iris[, 5])
```

## 二次判别分析 {#sec-quadratic-discriminant-analysis}

```{r}
# qda
iris_qda <- qda(Species ~ ., data=iris)
iris_qda
```

```{r}
table(predict(iris_qda, iris[, -5])$class, iris[, 5])
```

## 朴素贝叶斯 {#sec-naive-bayes}

```{r}
library(e1071) # 朴素贝叶斯
iris_nb <- naiveBayes(Species ~ ., data = iris)
iris_nb
# 分类结果
table(predict(iris_nb, iris), iris[,5])
```

## 支持向量机 {#sec-support-vector-machines}

```{r}
# e1071
iris_svm <- svm(Species ~ ., data = iris)
iris_svm
```

```{r}
library(kernlab)
iris_ksvm <- ksvm(Species ~ ., data = iris)
iris_ksvm
```

**kernlab** 包的绘图函数 `plot()` 仅支持二分类模型。

```{r}
iris_pred_svm <- predict(iris_ksvm, iris[, -5], type = "response")
table(iris_pred_svm, iris[, 5])
```

## K 最近邻 {#sec-k-nearest-neighbour}

```{r}
# 将 iris3 数据集拆分为训练集和测试集
iris_train <- rbind(iris3[1:25, , 1], iris3[1:25, , 2], iris3[1:25, , 3])
iris_test <- rbind(iris3[26:50, , 1], iris3[26:50, , 2], iris3[26:50, , 3])
iris_species <- factor(rep(c("setosa", "versicolor", "virginica"), each = 25))
```

```{r}
library(class)
# 分 3 类
iris_knn <- knn(
  train = iris_train, test = iris_test,
  cl = iris_species, k = 3, prob = TRUE
)
# 分类结果汇总
table(iris_knn, iris_species) 
```

## 神经网络 {#sec-neural-networks}

```{r}
library(nnet)
iris_nnet <- nnet(Species ~ ., data = iris, size = 4, trace = FALSE)
summary(iris_nnet)
```

size 隐藏层中的神经元数量

```{r}
iris_pred_nnet <- predict(iris_nnet, newdata = iris[,-5], type = "class")
table(iris_pred_nnet, iris[, 5])
```

## 决策树 {#sec-recursive-partitioning}

```{r}
library(rpart)
iris_rpart <- rpart(Species ~ ., data = iris)
iris_rpart
```

```{r}
#| label: fig-iris-rpart
#| fig-width: 5
#| fig-height: 4
#| fig-cap: 分类回归树
#| fig-showtext: true

library(rpart.plot)
rpart.plot(iris_rpart)
```

```{r}
iris_pred_rpart <- predict(iris_rpart, iris[, -5], type = "class")
table(iris_pred_rpart, iris[, 5])
```

## 随机森林 {#sec-random-forests}

```{r}
library(randomForest) # 随机森林
iris_rf <- randomForest(
  Species ~ ., data = iris,
  importance = TRUE, proximity = TRUE
)
# 分类结果
print(iris_rf)
```

```{r}
#| label: fig-iris-rf
#| fig-cap: 随机森林
#| fig-height: 4
#| fig-width: 5
#| fig-showtext: true
#| code-fold: true
#| echo: !expr knitr::is_html_output()

op <- par(mar = c(4, 4, 1.5, 0.1))
plot(iris_rf, main = "")
on.exit(par(op), add = TRUE)
```

```{r}
#| label: fig-iris-vi
#| fig-cap: 变量重要性
#| fig-height: 4
#| fig-width: 7
#| fig-showtext: true

varImpPlot(iris_rf, main = "变量重要性")
```

```{r}
iris_pred_rf <- predict(iris_rf, iris[, -5], type = "response")
table(iris_pred_rf, iris[, 5])
```

## 集成学习

在训练模型之前，需要先对数据集做预处理，包括分组采样、类别编码、数据拆分、类型转换等。

制作一个函数对数据集添加新列 `mark` 作为训练集 train 和测试集 test 的采样标记，返回数据。

```{r}
# 输入数据 x 和采样比例 prop
add_mark <- function(x = iris, prop = 0.7) {
  idx <- sample(x = nrow(x), size = floor(nrow(x) * prop))
  rbind(
    cbind(x[idx, ], mark = "train"),
    cbind(x[-idx, ], mark = "test")
  )
}
```

为了使采样结果可重复，设置随机数种子，然后对 `iris` 数据集按列 `Species` 分组添加采样标记，分组随机抽取 70% 的样本作为训练数据，余下的作为测试数据。就 `iris` 数据集来说，训练集有 `35*3 = 105` 条记录，测试集有 `15*3 = 45` 条记录。

```{r}
set.seed(20232023)
iris_df <- do.call(rbind, lapply(split(iris, iris$Species), add_mark, prop = 0.7))
```

为了使用函数 `fcase()` 对分类变量 `Species` 做重编码操作，加载 **data.table** 包，将数据集 `iris_df` 转为 `data.table` 类型。值得注意，**xgboost** 包要求分类变量的类别序号必须从 0 开始。

```{r}
# 数据准备
library(data.table)
iris_dt <- as.data.table(iris_df)
iris_dt <- iris_dt[, Species := fcase(
  Species == "setosa", 0,
  Species == "versicolor", 1,
  Species == "virginica", 2
)]
```

将数据 `iris_dt` 拆分成训练集和测试集，并以列表结构存储数据，样本数据及标签以矩阵类型存储。

```{r}
# 训练数据
iris_train <- list(
  data = as.matrix(iris_dt[iris_dt$mark == "train", -c("mark", "Species")]),
  label = as.matrix(iris_dt[iris_dt$mark == "train", "Species"])
)
# 测试数据
iris_test <- list(
  data = as.matrix(iris_dt[iris_dt$mark == "test", -c("mark", "Species")]),
  label = as.matrix(iris_dt[iris_dt$mark == "test", "Species"])
)
```

数据准备好后，加载 **xgboost** 包，设置训练参数，开始训练分类模型。此分类任务中类别超过 2，是多分类任务，学习任务是分类，目标函数可以是 `objective = "multi:softprob"` 或者 `objective = "multi:softmax"`，相应的评估指标可以是 `eval_metric = "mlogloss"` 或者 `eval_metric = "merror"`。`iris` 数据集的分类变量 `Species` 共有 3 类，所以 `num_class = 3` 。

```{r}
library(xgboost)
iris_xgb <- xgboost(
  data = iris_train$data, 
  label = iris_train$label,
  objective = "multi:softmax",  # 学习任务
  eval_metric = "mlogloss",     # 评估指标
  nrounds = 2,   # 提升迭代的最大次数
  num_class = 3  # 分类数
)
```

将训练好的模型放在测试集数据上进行预测。

```{r}
# ?predict.xgb.Booster
iris_pred <- predict(object = iris_xgb, newdata = iris_test$data)
```

将预测结果与测试集中的样本标签对比，检查分类效果。

```{r}
table(iris_test$label, iris_pred)
```

## 总结 {#sec-classification-problems-summary}

不同的分类算法分布在不同的 R 包中，在使用方式上既有相通之处，又有不同之处。下表对多个 R 包的使用做了归纳。R 包之间的不一致性，计算预测分类的概率的语法。

| 函数           | R 包           | 代码                                       |
|:---------------|:---------------|:---------------------------------------|
| `lda()`        | **MASS**       | `predict(obj)`                             |
| `glm()`        | **stats**      | `predict(obj, type = "response")`          |
| `gbm()`        | **gbm**        | `predict(obj, type = "response", n.trees)` |
| `mda()`        | **mda**        | `predict(obj, type = "posterior")`         |
| `rpart()`      | **rpart**      | `predict(obj, type = "prob")`              |
| `Weka()`       | **RWeka**      | `predict(obj, type = "probability")`       |
| `logitboost()` | **LogitBoost** | `predict(obj, type = "raw", nIter)`        |
| `pamr.train()` | **pamr**       | `pamr.predict(obj, type = "posterior")`    |

## 习题 {#sec-exercise-classification}

1.  [**titanic**](https://github.com/paulhendricks/titanic) 包整理了来自 kaggle 的 [Titanic](https://www.kaggle.com/c/titanic/data) 数据集，详细记录了 891 位乘客的信息，它比 Base R 内置的 Titanic 数据集更加原始，细节更多，信息更加丰富。原数据集拆分为训练集 `titanic_train` 和测试集 `titanic_test`。因为有每个乘客的原始信息，我们可以在个体水平上建模，采用更加复杂的模型分析泰坦尼克号乘客存活率及其影响因素。
