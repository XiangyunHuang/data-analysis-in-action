# 分类问题 {#sec-classification-problems}

```{r}
#| message: false

library(nnet) # 多项回归 multinom
library(glmnet) # 多项回归
library(kernlab) # 基于核的支持向量机 ksvm
library(e1071) # 朴素贝叶斯 naiveBayes 和支持向量机 svm
library(rpart) # 决策树 rpart
library(MASS) # 线性判别分析 lda
library(randomForest) # 随机森林 randomForest
library(class) # K 最近邻 knn
library(xgboost) # 集成学习
library(lattice)
```

以 iris 数据集为例，简单，方便模型和算法介绍，定位入门。分类间隔最大化，也是一个优化问题，找一条分界线，一个分割面，一个超平面划分不同的种类。本章篇幅：每个算法 4 页，共计 32 页。8 个算法的介绍按照分类思路，模型，代码和参数说明，分类性能评估。应用案例是手写数字识别。要点不是数据如何复杂，而是怎样把理论写得通俗、准确，看了之后能够应用到复杂的真实数据分析场景中去。理论解释、绘图说明、经验总结。

-   线性分类器

    -   多项回归模型
    -   线性判别分析

-   非线性分类器

    -   二次判别分析

    -   朴素贝叶斯

    -   支持向量机

    -   K 最近邻

    -   神经网络

    -   决策树

    -   随机森林

    -   集成学习

## 多项回归模型 {#sec-multinomial-regression-models}

```{r}
library(nnet) # 多项逻辑回归
iris_multinom <- multinom(Species ~ ., data = iris, trace = FALSE)
summary(iris_multinom)
```

```{r}
table(predict(iris_multinom, iris[, -5], type = "class"), iris[, 5])
```

在有的数据中，观测变量之间存在共线性，采用变量选择方法，比如 Lasso 方法压缩掉一部分变量。

```{r}
library(glmnet) # 多项回归
iris_glmnet <- glmnet(x = iris[, -5], y = iris[, 5], family = "multinomial")
```

```{r}
#| label: fig-multinom-glmnet
#| fig-cap: "回归系数的迭代路径"
#| fig-width: 5
#| fig-height: 5
#| fig-showtext: true

plot(iris_glmnet)
```

```{r}
#| label: fig-multinom-lambda
#| fig-cap: "惩罚系数的迭代路径"
#| fig-width: 5
#| fig-height: 5
#| fig-showtext: true

plot(iris_glmnet$lambda,
  ylab = expression(lambda), xlab = "迭代次数",
  main = "惩罚系数的迭代路径"
)
```

选择一个迭代趋于稳定时的 lambda，比如 `iris_glmnet$lambda[80]` 。

```{r}
coef(iris_glmnet, s = 0.0002796185)
```

```{r}
iris_pred_glmnet <- predict(
  object = iris_glmnet, newx = as.matrix(iris[, -5]),
  s = 0.0002796185, type = "class"
)
```

```{r}
table(iris_pred_glmnet, iris[, 5])
```

## 线性判别分析 {#sec-linear-discriminant-analysis}

```{r}
library(MASS)
# lda
iris_lda <- lda(Species ~ ., data=iris)
iris_lda
```

```{r}
table(predict(iris_lda, iris[, -5])$class, iris[, 5])
```

## 二次判别分析 {#sec-quadratic-discriminant-analysis}

```{r}
# qda
iris_qda <- qda(Species ~ ., data=iris)
iris_qda
```

```{r}
table(predict(iris_qda, iris[, -5])$class, iris[, 5])
```

## 朴素贝叶斯 {#sec-naive-bayes}

```{r}
library(e1071) # 朴素贝叶斯
iris_nb <- naiveBayes(Species ~ ., data = iris)
iris_nb
# 分类结果
table(predict(iris_nb, iris), iris[,5])
```

## 支持向量机 {#sec-support-vector-machines}

```{r}
# e1071
iris_svm <- svm(Species ~ ., data = iris)
iris_svm
```

```{r}
library(kernlab)
iris_ksvm <- ksvm(Species ~ ., data = iris)
iris_ksvm
```

**kernlab** 包的绘图函数 `plot()` 仅支持二分类模型。

```{r}
iris_pred_svm <- predict(iris_ksvm, iris[, -5], type = "response")
table(iris_pred_svm, iris[, 5])
```

## K 最近邻 {#sec-k-nearest-neighbour}

```{r}
# 将 iris3 数据集拆分为训练集和测试集
iris_train <- rbind(iris3[1:25, , 1], iris3[1:25, , 2], iris3[1:25, , 3])
iris_test <- rbind(iris3[26:50, , 1], iris3[26:50, , 2], iris3[26:50, , 3])
iris_species <- factor(rep(c("setosa", "versicolor", "virginica"), each = 25))
```

```{r}
library(class)
# 分 3 类
iris_knn <- knn(
  train = iris_train, test = iris_test,
  cl = iris_species, k = 3, prob = TRUE
)
# 分类结果汇总
table(iris_knn, iris_species) 
```

## 神经网络 {#sec-neural-networks}

```{r}
library(nnet)
iris_nnet <- nnet(Species ~ ., data = iris, size = 4, trace = FALSE)
summary(iris_nnet)
```

size 隐藏层中的神经元数量

```{r}
iris_pred_nnet <- predict(iris_nnet, newdata = iris[,-5], type = "class")
table(iris_pred_nnet, iris[, 5])
```

## 决策树 {#sec-recursive-partitioning}

```{r}
library(rpart)
iris_rpart <- rpart(Species ~ ., data = iris)
iris_rpart
```

```{r}
#| label: fig-iris-rpart
#| fig-width: 5
#| fig-height: 4
#| fig-cap: 分类回归树
#| fig-showtext: true

library(rpart.plot)
rpart.plot(iris_rpart)
```

```{r}
iris_pred_rpart <- predict(iris_rpart, iris[, -5], type = "class")
table(iris_pred_rpart, iris[, 5])
```

## 随机森林 {#sec-random-forests}

```{r}
library(randomForest) # 随机森林
iris_rf <- randomForest(
  Species ~ ., data = iris,
  importance = TRUE, proximity = TRUE
)
# 分类结果
print(iris_rf)
```

```{r}
#| label: fig-iris-rf
#| fig-cap: 随机森林
#| fig-height: 4
#| fig-width: 5
#| fig-showtext: true
#| code-fold: true
#| echo: !expr knitr::is_html_output()

op <- par(mar = c(4, 4, 1.5, 0.1))
plot(iris_rf, main = "")
on.exit(par(op), add = TRUE)
```

```{r}
#| label: fig-iris-vi
#| fig-cap: 变量重要性
#| fig-height: 4
#| fig-width: 7
#| fig-showtext: true

varImpPlot(iris_rf, main = "变量重要性")
```

```{r}
iris_pred_rf <- predict(iris_rf, iris[, -5], type = "response")
table(iris_pred_rf, iris[, 5])
```

## 集成学习

梯度提升

```{r}
library(xgboost)
```
