# 统计计算 {#sec-statistical-computation}



## 优化问题与回归问题 {#sec-regression-optimization}

<!-- 
广义最小二乘拟合 nlme::gls  预测 nlme::predict.gls
用广义最小二乘拟合线性模型 MASS::lm.gls	
用广义最小二乘拟合趋势面 spatial::surf.gls

[nlsr](https://cran.r-project.org/package=nlsr)
[bestsubset](https://github.com/ryantibs/best-subset/) 最优子集回归
[abess](https://github.com/abess-team/abess)
-->



1996 年出现 Lasso [@lasso1996]，由于缺少高效的求解算法，Lasso 在高维小样本特征选择研究中没有广泛流行，最小角回归(Least Angle Regression, LAR)算法 [@lar2004] 的出现有力促进了Lasso在高维小样本数据中的应用。为了解决Lasso的有偏估计问题，自适应 Lasso、松弛 Lasso， SCAD (Smoothly Clipped Absolute Deviation)[@scad2008]，MCP (Minimax Concave Penalty)[@mcp2010] 陆续出现。经典的普通最小二乘、广义最小二乘、岭回归、逐步回归、Lasso 回归、最优子集回归都可转化为优化问题，一般形式如下：

$$
\underbrace{\hat{\theta}_{\lambda_n}}_{\text{待估参数}} \in \arg \min_{\theta \in \Omega} \left\{ \underbrace{\mathcal{L}(\theta;Z_{1}^{n})}_{\text{损失函数}} + \lambda_n \underbrace{\mathcal{R}(\theta)}_{\text{正则化项}} \right\}.
$$

下面以 [**nloptr**](https://github.com/astamm/nloptr) 包的优化器来展示求解过程，并与 Base R、[**glmnet**](https://glmnet.stanford.edu/) 和 [**CmdStanR**](https://github.com/stan-dev/cmdstanr) 实现的回归模型比较。

<!-- 向量用小写，矩阵用大写 -->

$$
\arg \min_{\beta,\lambda} ~~ \frac{1}{2} || \mathbf{y} - \mathbf{X} \beta ||_2^2 +  \lambda ||\beta||_1
$$

其中，$X \in \mathbb{R}^{m\times n}$， $y \in \mathbb{R}^m$，$\beta \in \mathbb{R}^n$， $0 < \lambda \in \mathbb{R}$ 。



## 对数似然与损失函数 {#sec-log-likelihood}

随机变量 X 服从参数为 $\lambda > 0$ 的指数分布，密度函数 $p(x)$ 为

$$
\begin{array}{l}
 p(x) = \left\{ 
    \begin{array}{l}
    \lambda\mathrm{e}^{-\lambda x}, ~ x \geq 0 \\
    0, \quad x < 0
    \end{array} \right.
\end{array}
$$

其中，$\lambda > 0$，下面给定一系列模拟样本观察值 $x_1, x_2, \cdots, x_n$，估计参数 $\lambda$。对数似然函数 $\ell(\lambda) = \log \prod_{i=1}^{n} f(x_i) = n \log \lambda - \lambda \sum_{i=1}^{n}x_i$。解此方程即可得到 $\lambda$ 的极大似然估计 $\lambda_{mle} = \frac{1}{\bar{X}}$，极大值 $\ell(\lambda_{mle}) = - n(1 + \log \bar{X})$。

根据上述样本，计算样本均值 $(\mu - 1.5*\sigma/\sqrt{n}, \mu + 1.5*\sigma/\sqrt{n})$ 和方差 $(0.8\sigma, 1.5\sigma)$。

已知正态分布 $f(x) = \frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{- \frac{(x - \mu)^2}{2\sigma^2}}$ 的对数似然形式如下：

$$
\ell(\mu,\sigma^2) = \log \prod_{i=1}^{n} f(x_i) = \sum_{i=1}^{n}\log f(x_i)
$$ {#eq-log-lik-norm}

参数 $\mu$ 和 $\sigma^2$ 的极大似然估计为

在 R 语言中，正态分布的密度函数的对数可用 `dnorm(..., log = TRUE)` 计算。

生成服从指数分布的样本，计算样本的均值和方差，依据均值和方差构造区间，然后将区间网格化，在此网格上绘制正态分布的对数似然函数。即不知道正态分布的参数，将从指数分布模拟出来的样本用正态分布拟合。

<!-- 绕那么大一个圈子，其实就是绘制正态分布的对数似然函数。 -->

```{r}
#| label: fig-log-likelihood
#| fig-cap: "正态分布参数的负对数似然函数"
#| fig-width: 5
#| fig-height: 4.5
#| fig-showtext: true

set.seed(2021)
n <- 20 # 随机数的个数
x <- rexp(n, rate = 5) # 服从指数分布的随机数
m <- 40 # 网格数
mu <- seq(
  mean(x) - 1.5 * sd(x) / sqrt(n),
  mean(x) + 1.5 * sd(x) / sqrt(n),
  length.out = m
)
sigma <- seq(0.8 * sd(x), 1.5 * sd(x), length.out = m)
df <- expand.grid(x = mu, y = sigma)
# 正态分布的对数似然
loglik <- function(b, x0) -sum(dnorm(x0, b[1], b[2], log = TRUE))

df$fnxy <- apply(df, 1, loglik, x0 = x)
library(lattice)
wireframe(
  data = df, fnxy ~ x * y,
  shade = TRUE, drape = FALSE,
  xlab = expression(mu),
  ylab = expression(sigma),
  zlab = list(expression(-loglik(mu, sigma)), rot = 90),
  scales = list(arrows = FALSE, col = "black"),
  # 减少三维图形的边空
  lattice.options = list(
    layout.widths = list(
      left.padding = list(x = -.6, units = "inches"),
      right.padding = list(x = -1.0, units = "inches")
    ),
    layout.heights = list(
      bottom.padding = list(x = -.8, units = "inches"),
      top.padding = list(x = -1.0, units = "inches")
    )
  ),
  par.settings = list(axis.line = list(col = "transparent")),
  screen = list(z = 120, x = -70, y = 0)
)
```


<!-- 
添加极大值点，除指数分布外，还有正态、二项、泊松分布观察其似然曲面的特点，都是单峰，有唯一极值点，再考虑正态混合模型的似然曲面 

逻辑回归写得很好
[Getting the most out of logistic regression](https://gongcastro.github.io/blog/logistic-regression/logistic-regression.html)
频率派+正则 = 贝叶斯+先验
在极大似然估计的框架下，以线性模型为例，贝叶斯估计和惩罚极大似然估计是等价的，先验分布的形式就是惩罚函数的形式，如正态先验对应于 L2 惩罚。
L2 惩罚对应正态先验、L1 惩罚对应拉普拉斯先验
https://github.com/jgabry/bayes-workflow-book/blob/master/bayesian-estimation.Rmd
Stan 入门写得很好
[Getting Started with Stan](https://github.com/LuZhangstat/Getting-Started-with-Stan)
-->

## 贝叶斯计算框架 Stan {#sec-bayesian-computation-stan}

[Stan](https://github.com/stan-dev/stan) 是一款贝叶斯计算软件，定义了一套概率编程语言，提供 R、Python、Matlab 语言等众多的编程接口，[CmdStan](https://github.com/stan-dev/cmdstan) 是其命令行编程接口，与 Stan 版本保持同步，[CmdStanR](https://github.com/stan-dev/cmdstanr) 包集成 CmdStan 软件，可以非常方便地分析运行结果。下面以逻辑回归模型为例，介绍 CmdStan 框架的使用。


```{r}
#| label: fig-logistic
#| echo: false
#| fig-cap: "逻辑斯谛分布"
#| fig-subcap: 
#| - 概率密度函数
#| - 概率分布函数
#| fig-width: 4
#| fig-height: 3
#| fig-ncol: 2
#| fig-showtext: true

library(ggplot2)
ggplot() +
  geom_function(
    fun = dlogis, args = list(location = 0, scale = 0.5),
    colour = "#E41A1C", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  geom_function(
    fun = dlogis, args = list(location = 0, scale = 1),
    colour = "#377EB8", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  geom_function(
    fun = dlogis, args = list(location = 0, scale = 2),
    colour = "#4DAF4A", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  theme_classic() +
  labs(x = expression(x), y = expression(f(x)))

ggplot() +
  geom_function(
    fun = plogis, args = list(location = 0, scale = 0.5),
    colour = "#E41A1C", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  geom_function(
    fun = plogis, args = list(location = 0, scale = 1),
    colour = "#377EB8", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  geom_function(
    fun = plogis, args = list(location = 0, scale = 2),
    colour = "#4DAF4A", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  theme_classic() +
  labs(x = expression(x), y = expression(bolditalic(F)(x)))
```

响应变量 $Y$ 服从伯努利分布 $\mathrm{Bernoulli}(p)$，取值是 0 或 1，对线性预测 $X^{\top}\boldsymbol{\beta}$ 做 Logistic 变换

$$
\mathrm{E}Y = p = \mathrm{Logistic}(X^{\top}\boldsymbol{\beta}) = \frac{1}{1 + e^{-(\alpha + X^{\top}\boldsymbol{\beta})}}
$$

Logistic 的逆变换

$$
\mathrm{Logistic}^{-1}(p)= \ln\big(\frac{p}{1-p}\big) = \alpha + X^{\top}\boldsymbol{\beta}
$$

记数据矩阵 $X$ 为

$$
X = \begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{k1} & x_{k2} & x_{k3} & \dots  & x_{kn}
\end{bmatrix}
$$

记 $X^{\top} = (x_1, x_2, \cdots, x_n)^{\top}$ 是一个$n \times k$矩阵，则 $x_i^{\top}$ 表示数据矩阵 $X^{\top}$ 的第 $i$ 行，一共有 $k$ 列。同理 $\boldsymbol{\beta} = (\beta_1, \beta_2, \cdots, \beta_k)^{\top}$ ，$\boldsymbol{\beta}$ 是一个 $k \times 1$ 的矩阵，其中 $j = 1, \cdots, k$ ，对第 $i$ 次观测

$$
\mathrm{Logistic}^{-1}(p_i)= \ln\big(\frac{p_i}{1-p_i}\big) = \alpha + x_i^{\top}\boldsymbol{\beta}
$$

其中 $x_i^{\top}$ 表示 $1 \times k$ 的矩阵。关于参数 $\alpha,\boldsymbol{\beta}$ 的似然函数如下：

$$
\begin{aligned}
L(\alpha,\boldsymbol{\beta}) &= \prod_{i=1}^{n} p_i^{y_i}(1 - p_i)^{1 - y_i} \\ 
     &= \prod_{i=1}^{n} \Big(\frac{e^{\alpha + x_i^{\top}\boldsymbol{\beta}}}{1 + e^{\alpha + x_i^{\top}\boldsymbol{\beta}}}\Big)^{y_i}\Big(\frac{1}{e^{\alpha + x_i^{\top}\boldsymbol{\beta}}}\Big)^{1-y_i} \\
\end{aligned}
$$

关于参数 $\alpha,\boldsymbol{\beta}$ 的对数似然函数如下：

$$
\begin{aligned}
\ell(\alpha,\boldsymbol{\beta}) &= \log L(\alpha,\boldsymbol{\beta}) \\
& = \sum_{i=1}^{n} \Big[y_i \log (p_i) + (1 - y_i) \log(1-p_i)\Big] \\
&= \sum_{i=1}^{n} \Big[y_i \log \Big(\frac{e^{\alpha + x_i^{\top}\boldsymbol{\beta}}}{1 + e^{\alpha + x_i^{\top}\boldsymbol{\beta}}}\Big) + (1 - y_i) \log\Big(\frac{1}{e^{\alpha + x_i^{\top}\boldsymbol{\beta}}}\Big)\Big]
\end{aligned}
$$ {#eq-log-logit-lik}

对数似然函数 $\ell(\alpha,\boldsymbol{\beta})$ 关于参数 $\alpha,\boldsymbol{\beta}$ 的偏导数如下：

$$
\begin{aligned}
\frac{\partial \ell(\alpha,\boldsymbol{\beta})}{\partial \alpha}  &= \sum_{i=1}^{n}\Big[(2y_i -1) - y_i \frac{e^{\alpha + x_i^{\top}\boldsymbol{\beta}}}{1 + e^{\alpha + x_i^{\top}\boldsymbol{\beta}}} \Big] \\
\frac{\partial \ell(\alpha,\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &= \sum_{i=1}^{n}\Big[(2y_i - 1) - y_i \frac{e^{\alpha + x_i^{\top}\boldsymbol{\beta}}}{1 + e^{\alpha + x_i^{\top}\boldsymbol{\beta}}} \Big] x_i
\end{aligned}
$$ {#eq-log-logit-lik-partial}

其中， $\sum_{i=1}^{n} y_i$ 表示 $y_i = 1$ 的观测值数量，$(n - \sum_{i=1}^{n}y_i)$ 表示 $y_i = 0$ 的观测值数量，一般通过拟牛顿法可以求解此优化问题。

从一个逻辑回归模型模拟一组样本，共 2500 条记录，即 $n = 2500$，10 个观测变量，即 $k=10$，其中，只有变量 $X_1$ 和 $X_2$ 的系数非零，参数设定为 $\alpha = 1, \beta_1 = 3,\beta_2 = -2$，而 $\beta_i = 0, i=3, \cdots, 10$ 模拟数据的代码如下：

```{r}
set.seed(2023)
n <- 2500
k <- 10
X <- matrix(rnorm(n * k), ncol = k)
y <- rbinom(n, size = 1, prob = plogis(1 + 3 * X[,1] - 2 * X[,2]))
```

模拟数据矩阵 X 与上述记号 $X^{\top}$ 是对应的，记号 $x_i^{\top}$ 表示数据矩阵的第 $i$ 行。

极大化对数似然函数 @eq-log-logit-lik ，就是求解一个多维非线性无约束优化问题。

```{r}
#| eval: false
# alpha 是截距
# beta 是 k 维向量
# X 是 n x k 维的矩阵且 n > k
# y 是 n 维向量
# 目标函数
# optim 求极小，因此对数似然函数前添加负号
# alpha 合并进 beta 
log_logit_lik <- function(beta) {
  -sum(y * plogis(cbind(1, X) %*% beta, log.p = T) - (1 - y) * log(1 - plogis(cbind(1, X) %*% beta)))
}
```

当用 Base R 函数 `optim()` 来求解时，发现陷入局部最优解。

```{r}
# 11 维非线性无约束优化
# Nelder-Mead
optim(
  par = rep(1, 11),   # 初始值
  fn = log_logit_lik, # 目标函数
  method = "Nelder-Mead"
)
```

下面用 **nloptr** 包来求解。

```{r}
#| eval: false
# 加载 ROI 时不要自动加载插件
Sys.setenv(ROI_LOAD_PLUGINS = FALSE)
library(ROI)
library(ROI.plugin.nloptr)
op <- OP(
  objective = F_objective(F = log_logit_lik, n = 11L),
  types = rep("C", 11), maximum = FALSE,
  bounds = V_bound(ld = -3, ud = 3, nobj = 11L)
)
nlp <- ROI_solve(op, solver = "nloptr.directL")
nlp$solution
```

```{r}
#| eval: false
# 梯度函数
log_logit_lik_grad <- function(beta) {
  - rbind(1, t(X)) %*% ((2 * y - 1) - y * plogis(cbind(1, X) %*% beta) )
}

optim(
  par = rep(0, 11),   # 初始值
  fn = log_logit_lik, # 目标函数
  gr = log_logit_lik_grad, # 目标函数的梯度
  method = "L-BFGS-B"
)
```

如果对数似然函数是多模态的，一般的求解器容易陷入局部最优解，推荐用 **nloptr** 包的[全局优化求解器](https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#global-optimization)。

### `glm.fit()` {#sec-glm-fit}

Base R 提供的函数 `glm.fit()` 拟合模型，指定联系函数为 logit 变换

```{r}
fit_r <- glm.fit(x = cbind(1, X), y = y, family = binomial(link = "logit"))
coef(fit_r)
```



### glmnet {#sec-logit-glmnet}

调用 **glmnet** 包的函数 `glmnet()` 拟合模型，指定指数族的具体形式为二项分布，伯努利分布是二项分布的特殊形式，也叫两点分布或0-1分布。

```{r}
#| message: false

library(Matrix)
library(glmnet)
fit_glm <- glmnet(x = X, y = y, family = "binomial")
```

逻辑回归模型系数在 L1 正则下的迭代路径图

```{r}
#| label: fig-logit-glmnet
#| fig-cap: "回归系数的迭代路径"
#| fig-width: 5
#| fig-height: 5
#| fig-showtext: true

plot(fit_glm, ylab = "回归系数")
```

从图可见，剩余两个系数是非零的，一个是 3， 一个是 -2，其余都被压缩，而接近为 0 了。

```{r}
#| label: fig-logit-glmnet-lambda
#| fig-cap: "惩罚系数的迭代路径"
#| fig-width: 5
#| fig-height: 5
#| fig-showtext: true

plot(fit_glm$lambda, ylab = expression(lambda), xlab = "迭代次数",
     main = "惩罚系数的迭代路径")
```

随着迭代的进行，惩罚系数 $\lambda$ 越来越小，接近于 0，这也是符合预期的，因为模型本来就是简单的逻辑回归，不带惩罚项。选择一个迭代趋于稳定时的 $\lambda$ 比如 0.0005247159，此时各个参数的取值如下：

```{r}
coef(fit_glm, s = 0.0005247159)
```

截距 (Intercept) 对应 $\alpha = 0.997741857$，而 $\beta_1 = 3.076358149$ 对应 V1，$\beta_2 = -1.984018387$ 对应 V2，以此类推。

### CmdStanR {#sec-logit-cmdstanr}

下面用 Stan 编码逻辑回归模型，模型代码如下：

```{verbatim, file="code/bernoulli_logit_glm.stan", lang="stan"}
```

CmdStan 已经安装了，接下来，用 **cmdstanr** 包调 CmdStan 拟合模型，有三步：其一根据上面 Stan 代码中数据块的定义准备输入数据，其二编译上面的 Stan 代码获得可执行的模型文件，其三将数据输送到可执行的模型文件进行抽样。

```{r}
#| label: compile-logit-model
#| message: false
#| results: hide

library(cmdstanr)
# 准备数据
mdata <- list(k = k, n = n, y = y, X = X)
# 来自 stan-dev/cmdstanr
mod_logit <- cmdstan_model(
  stan_file = "code/bernoulli_logit_glm.stan",
  compile = TRUE,
  cpp_options = list(stan_threads = TRUE)
)
fit_logit <- mod_logit$sample(
  data = mdata, 
  chains = 4, 
  parallel_chains = 1,
  iter_warmup = 1000, # 每条链预处理迭代次数
  iter_sampling = 2000, # 每条链总迭代次数
  threads_per_chain = 1, # 每条链设置一个线程
  seed = 20232023,
  show_messages = FALSE, 
  refresh = 0
)
```

模型拟合结果存储为一个 [**R6**](https://github.com/r-lib/R6) 类型的数据对象，调用 summary 方法可以获得任意参数的结果，见 @tbl-logit-output 。

```{r}
#| label: tbl-logit-output
#| tbl-cap: "模型参数的贝叶斯估计结果"
#| echo: false

fit_logit$summary(c("alpha", "beta", "lp__")) |> 
  knitr::kable(digits = 3)
```

贝叶斯方法估计模型参数是通过抽样获得的，抽样的过程是参数迭代的过程，如果迭代过程是收敛的，估计值最终会收敛到参数的真值。一般来说，我们希望迭代的效率越高越好，只需要少量的迭代次数和短暂的时间就可以获得很好的效果。接下来，看看采样过程的诊断结果：

```{r}
fit_logit$cmdstan_diagnose()
```

每个参数都有迭代轨迹，有效样本数 effective sample size $n_{eff}$，
潜在尺度缩减因子 potential scale reduction factor $\hat{R}$，采样效率可以理解为每秒迭代产生有效样本数。

```{r}
# 简化版本
fit_logit$diagnostic_summary()
```

数据是根据给定模型生成的，数据与模型吻合得很好，迭代过程没有任何发散的情况。还可以可视化的方式检查参数的迭代情况，**bayesplot** 包提供大量的可视化函数检查迭代过程和结果，逻辑回归模型的参数 $\beta_1$ 和 $\beta_2$ 的轨迹图如下：

```{r}
#| label: fig-post-logit-trace
#| fig-cap: "参数的迭代轨迹"
#| fig-showtext: true
#| fig-width: 5
#| fig-height: 6
#| message: false

library(ggplot2)
library(bayesplot)
mcmc_trace(fit_logit$draws(c("beta[1]", "beta[2]")),
  facet_args = list(
    labeller = ggplot2::label_parsed,
    strip.position = "top",
    ncol = 1
  )
) + theme_classic()
```

马尔科夫链蒙特卡罗采样的后验分布图如下：

```{r}
#| label: fig-post-logit-hist
#| fig-cap: "参数的后验分布"
#| fig-showtext: true
#| fig-width: 5
#| fig-height: 6
#| message: false

mcmc_hist(fit_logit$draws(c("beta[1]", "beta[2]")),
  facet_args = list(
    labeller = ggplot2::label_parsed,
    strip.position = "top",
    ncol = 1
  )
) + theme_classic()
```

图检验的其他方法还有轨迹图 `mcmc_trace()`，散点图 `mcmc_scatter()`，密度图 `mcmc_dens()`，直方图 `mcmc_hist()`，自相关图 `mcmc_acf()`，区间图 `mcmc_intervals()` 和岭线图 `mcmc_areas_ridges()` 等。


CmdStanR 的 sample 方法默认的采样器为 NUTS 的 HMC 算法，CmdStanR 的封装导致不能修改采样器，读者若想修改，Stan 的命令行接口 CmdStan 支持修改采样器，不过推荐读者使用默认的 NUTS 采样器，它是根据动态调整来决定 optimal integration time。NUTS 采样器的参数 `max_depth = 10` 是可以调整的，采样的树深度。

除了 sample 方法还有优化 optimize （L-BFGS 算法）和变分推断 variational 两个优化器。

```{r}
#| results: false
# L-BFGS 算法拟合模型
fit_optim_logit <- mod_logit$optimize(
  data = mdata, # 观测数据
  init = 0, # 所有参数初值设为 0 
  refresh = 0, # 不显示迭代进程
  algorithm = "lbfgs", # 优化器
  threads = 1, # 单线程
  seed = 20232023 # 随机数种子
)
```

模型输出结果如下：

```{r}
fit_optim_logit$summary(c("alpha", "beta", "lp__"))
```

Stan 实现的 variational 方法调用自动微分变分近似推断优化器（ADVI 算法）获取参数的后验分布。

```{r}
#| results: false
# 自动微分变分近似推断
fit_advi_logit <- mod_logit$variational(
  data = mdata, # 观测数据
  init = 0, # 所有参数初值设为 0 
  refresh = 0, # 不显示迭代进程
  algorithm = "meanfield", # 优化器
  threads = 1, # 单线程
  seed = 20232023 # 随机数种子
)
```

模型输出结果见下 @tbl-logit-advi 。

```{r}
#| label: tbl-logit-advi
#| tbl-cap: "模型参数的 ADVI 结果"
#| echo: false

fit_advi_logit$summary(c("alpha", "beta", "lp__")) |> 
  knitr::kable(digits = 3)
```

::: {.callout-tip}
CmdStanR 始终与 CmdStan 保持同步更新，及时地用上集成到 Stan 当中的学术研究成果。而 RStan 及其衍生包 rstanarm 和 brms 等依赖太重，安装、更新都比较麻烦。入门 CmdStanR 后，可以快速转入对 Stan 底层原理的学习，有利于编码符合实际需要的复杂模型，有利于掌握常用的炼丹技巧，提高科研和工作的效率。
:::


## 机器学习框架 tensorflow {#sec-machine-learning-tensorflow}

### 软件配置 {#sec-setup-tensorflow}

推荐读者配置独立的 Python 虚拟环境，在虚拟环境中来做算法开发，下面用 virtualenv 配置一个开发环境。

如果读者是 MacOS 系统，推荐安装命令行工具 Command Line Tools for Xcode，它提供大量常用的开发工具，如 Apple LLVM compiler, linker, and Make，Git，Python 等。安装过程非常简单，只需如下一行命令。

```bash
xcode-select --install
```

也可以在网上先下载[工具包](https://developer.apple.com/download/all/)，再离线安装。最近的 MacOS 系统已经自带了 Python 3.9.x，所以，只需在系统中安装 [virtualenv](https://github.com/pypa/virtualenv) 软件。推荐使用软件包管理工具[brew](https://github.com/Homebrew/brew)来安装。

```bash
brew install virtualenv
```

用 virtualenv 创建虚拟环境，虚拟环境的存放路径是 `/opt/.virtualenvs/r-tensorflow`，所以名字就是 `r-tensorflow`

```bash
# 准备虚拟环境萼存放地址
sudo mkdir -p /opt/.virtualenvs/r-tensorflow
# 赋予当前用户的读写权限
sudo chown -R $(whoami):staff /opt/.virtualenvs/r-tensorflow
# 方便后续复用
export RETICULATE_PYTHON_ENV=/opt/.virtualenvs/r-tensorflow
# 创建虚拟环境
virtualenv -p /usr/bin/python3 $RETICULATE_PYTHON_ENV
# 激活虚拟环境
source $RETICULATE_PYTHON_ENV/bin/activate
```

激活虚拟环境后，进入书籍仓库根目录，从指定文件 requirements.txt 安装 Python 模块。

```bash
pip install -r requirements.txt
```

安装完成后，可以在命令行中输入 `deactivate` 退出虚拟环境。至此，所需的 Python 开发环境已经准备好了。

接下来配置 R 环境，使得可以在 R 语言中调用 tensorflow 模块，搭建机器学习模型。在文件 `.Rprofile` 里设置环境变量 `RETICULATE_PYTHON` 和 `RETICULATE_PYTHON_ENV`，这样 [**reticulate**](https://github.com/rstudio/reticulate) 包就能发现和使用它了。

```bash
Sys.setenv(RETICULATE_PYTHON="/opt/.virtualenvs/r-tensorflow/bin/python")
Sys.setenv(RETICULATE_PYTHON_ENV="/opt/.virtualenvs/r-tensorflow")
```

最后，输入命令 `reticulate::py_config()` 检查配置情况，看到如下结果，表示配置成功。

```
python:         /opt/.virtualenvs/r-tensorflow/bin/python
libpython:      /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/config-3.9-darwin/libpython3.9.dylib
pythonhome:     /opt/.virtualenvs/r-tensorflow:/opt/.virtualenvs/r-tensorflow
virtualenv:     /opt/.virtualenvs/r-tensorflow/bin/activate_this.py
version:        3.9.6 (default, Oct 18 2022, 12:41:40)  [Clang 14.0.0 (clang-1400.0.29.202)]
numpy:          /opt/.virtualenvs/r-tensorflow/lib/python3.9/site-packages/numpy
numpy_version:  1.24.2

NOTE: Python version was forced by RETICULATE_PYTHON
```


::: {.callout-tip}
如果希望打开终端就进入虚拟环境，可以在 `~/.zshrc` 文件中添加两行：

```bash
export RETICULATE_PYTHON_ENV=/opt/.virtualenvs/r-tensorflow
source $RETICULATE_PYTHON_ENV/bin/activate
```
:::
