# 统计计算 {#sec-statistical-computation}

```{r}
#| echo: false

source("_common.R")
```

## 优化问题与回归问题 {#sec-regression-optimization}


1996 年出现 Lasso [@lasso1996]，由于缺少高效的求解算法，Lasso 在高维小样本特征选择研究中没有广泛流行，最小角回归(Least Angle Regression, LAR)算法 [@lar2004] 的出现有力促进了Lasso在高维小样本数据中的应用。为了解决Lasso的有偏估计问题，自适应 Lasso、松弛 Lasso， SCAD (Smoothly Clipped Absolute Deviation)[@scad2008]，MCP (Minimax Concave Penalty)[@mcp2010] 陆续出现。经典的普通最小二乘、广义最小二乘、岭回归、逐步回归、Lasso 回归、最优子集回归都可转化为优化问题，一般形式如下：

$$
\underbrace{\hat{\theta}_{\lambda_n}}_{\text{待估参数}} \in \arg \min_{\theta \in \Omega} \left\{ \underbrace{\mathcal{L}(\theta;Z_{1}^{n})}_{\text{损失函数}} + \lambda_n \underbrace{\mathcal{R}(\theta)}_{\text{正则化项}} \right\}.
$$

一个带 L1 惩罚项的回归模型的损失函数如下：

$$
\arg \min_{\boldsymbol{\beta},\lambda} ~~ \frac{1}{2} || \mathbf{y} - X \boldsymbol{\beta} ||_2^2 +  \lambda ||\boldsymbol{\beta}||_1
$$

其中，$X \in \mathbb{R}^{m\times n}$， $y \in \mathbb{R}^m$，$\beta \in \mathbb{R}^n$， $0 < \lambda \in \mathbb{R}$ 。

下面主要以逻辑回归模型为例，介绍 Base R、[**nloptr**](https://github.com/astamm/nloptr)、 [**glmnet**](https://glmnet.stanford.edu/) 和 [**CmdStanR**](https://github.com/stan-dev/cmdstanr) 等工具包提供的优化器，在求解性能、范围、方法等方面对比总结使用经验。

## 对数似然与损失函数 {#sec-log-likelihood}


### 线性回归 {#sec-linear-regression}



### 逻辑回归 {#sec-logistic-regression}


```{r}
#| label: fig-logistic
#| echo: false
#| fig-cap: "逻辑斯谛分布"
#| fig-subcap: 
#| - 概率密度函数
#| - 概率分布函数
#| fig-width: 4
#| fig-height: 3
#| fig-ncol: 2
#| fig-showtext: true

library(ggplot2)
ggplot() +
  geom_function(
    fun = dlogis, args = list(location = 0, scale = 0.5),
    colour = "#E41A1C", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  geom_function(
    fun = dlogis, args = list(location = 0, scale = 1),
    colour = "#377EB8", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  geom_function(
    fun = dlogis, args = list(location = 0, scale = 2),
    colour = "#4DAF4A", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  theme_classic() +
  labs(x = expression(x), y = expression(f(x)))

ggplot() +
  geom_function(
    fun = plogis, args = list(location = 0, scale = 0.5),
    colour = "#E41A1C", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  geom_function(
    fun = plogis, args = list(location = 0, scale = 1),
    colour = "#377EB8", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  geom_function(
    fun = plogis, args = list(location = 0, scale = 2),
    colour = "#4DAF4A", linewidth = 1.2, xlim = c(-6, 6)
  ) +
  theme_classic() +
  labs(x = expression(x), y = expression(bolditalic(F)(x)))
```

响应变量 $Y$ 服从伯努利分布 $\mathrm{Bernoulli}(p)$，取值是 0 或 1，对线性预测 $X\boldsymbol{\beta}$ 做 Logistic 变换

$$
p = \mathsf{E}Y = \mathrm{Logistic}(X\boldsymbol{\beta}) = \frac{1}{1 + e^{-(\alpha + X\boldsymbol{\beta})}} = \frac{e^{\alpha + X\boldsymbol{\beta}}}{1 + e^{\alpha + X\boldsymbol{\beta}}}
$$

Logistic 的逆变换

$$
\mathrm{Logistic}^{-1}(p)= \ln\big(\frac{p}{1-p}\big) = \alpha + X\boldsymbol{\beta}
$$

记数据矩阵 $X$ 为

$$
X = \begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1k} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2k} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & x_{n3} & \dots  & x_{nk}
\end{bmatrix}
$$

记 $X = (x_1, x_2, \cdots, x_n)$ 是一个$n \times k$矩阵，则 $x_i$ 表示数据矩阵 $X$ 的第 $i$ 行，一共有 $k$ 列。同理 $\boldsymbol{\beta} = (\beta_1, \beta_2, \cdots, \beta_k)$ ，对第 $i$ 次观测

$$
\mathrm{Logistic}^{-1}(p_i)= \ln\big(\frac{p_i}{1-p_i}\big) = \alpha + x_i\boldsymbol{\beta}
$$

其中 $x_i$ 是 $1 \times k$ 的矩阵，$\boldsymbol{\beta}$ 是一个 $k \times 1$ 的矩阵。关于参数 $\alpha,\boldsymbol{\beta}$ 的似然函数如下：

$$
\begin{aligned}
L(\alpha,\boldsymbol{\beta}) &= \prod_{i=1}^{n} p_i^{y_i}(1 - p_i)^{1 - y_i} \\ 
     &= \prod_{i=1}^{n} \Big(\frac{e^{\alpha + x_i\boldsymbol{\beta}}}{1 + e^{\alpha + x_i\boldsymbol{\beta}}}\Big)^{y_i}\Big(\frac{1}{e^{\alpha + x_i\boldsymbol{\beta}}}\Big)^{1-y_i} \\
\end{aligned}
$$ {#eq-logit-lik}

关于参数 $\alpha,\boldsymbol{\beta}$ 的对数似然函数如下：

$$
\begin{aligned}
\ell(\alpha,\boldsymbol{\beta}) &= \log L(\alpha,\boldsymbol{\beta}) \\
& = \sum_{i=1}^{n} \Big[y_i \log (p_i) + (1 - y_i) \log(1-p_i)\Big] \\
&= \sum_{i=1}^{n} \Big[y_i \log \Big(\frac{e^{\alpha + x_i\boldsymbol{\beta}}}{1 + e^{\alpha + x_i\boldsymbol{\beta}}}\Big) + (1 - y_i) \log\Big(\frac{1}{e^{\alpha + x_i\boldsymbol{\beta}}}\Big)\Big]
\end{aligned}
$$ {#eq-logit-lik-log}

对数似然函数 $\ell(\alpha,\boldsymbol{\beta})$ 关于参数 $\alpha,\boldsymbol{\beta}$ 的偏导数如下：

$$
\begin{aligned}
\frac{\partial \ell(\alpha,\boldsymbol{\beta})}{\partial \alpha}  &= \sum_{i=1}^{n}\Big[ \big(\frac{y_i}{p_i} -  \frac{1- y_i}{1 - p_i}\big) \frac{\partial p_i}{\partial \alpha} \Big] \\
\frac{\partial \ell(\alpha,\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &= \sum_{i=1}^{n}\Big[\big(\frac{y_i}{p_i} -  \frac{1- y_i}{1 - p_i}\big) \frac{\partial p_i}{\partial \beta} \Big] \\
& = \sum_{i=1}^{n}\Big[\big(\frac{y_i}{p_i} -  \frac{1- y_i}{1 - p_i}\big) p_i(1- p_i) x_i \Big] 
\end{aligned}
$$ {#eq-logit-lik-log-partial}

其中， $p_i = \frac{e^{\alpha + x_i\boldsymbol{\beta}}}{1 + e^{\alpha + x_i\boldsymbol{\beta}}}$ ，一般通过迭代加权最小二乘算法（Iteratively (Re-)Weighted Least Squares，简称 IWLS）求解此优化问题，它可以看作拟牛顿法的一种特殊情况。

## 数值优化问题求解器 {#sec-solvers}

### `optim()` {#sec-logit-optim}

从一个逻辑回归模型模拟一组样本，共 2500 条记录，即 $n = 2500$，10 个观测变量，即 $k=10$，其中，只有变量 $X_1$ 和 $X_2$ 的系数非零，参数设定为 $\alpha = 1, \beta_1 = 3,\beta_2 = -2$，而 $\beta_i = 0, i=3, \cdots, 10$ 模拟数据的代码如下：

```{r}
set.seed(2023)
n <- 2500
k <- 10
X <- matrix(rnorm(n * k), ncol = k)
y <- rbinom(n, size = 1, prob = plogis(1 + 3 * X[,1] - 2 * X[,2]))
```

模拟数据矩阵 X 与上述记号 $X$ 是对应的，记号 $x_i$ 表示数据矩阵的第 $i$ 行。$\alpha$ 是逻辑回归方程的截距，$\beta$ 是 $k$ 维向量，$X$ 是 $n \times k$ 维的矩阵且 $n > k$，$y$ 是 $n$ 维向量。极大化对数似然函数 @eq-logit-lik-log ，就是求解一个多维非线性无约束优化问题。方便起见，将 $\alpha$ 合并进 $\beta$ 向量，另，函数 `optim()` 默认求极小，因此在对数似然函数前添加负号。

```{r}
# 目标函数
log_logit_lik <- function(beta) {
  p <- plogis(cbind(1, X) %*% beta)
  -sum(y * log(p) + (1 - y) * log(1 - p))
}
```

高维情形下，没法绘制似然函数图形，退化到二维，如 @fig-log-logit-lik 所示，二维情形下的逻辑回归模型的负对数似然函数曲面。

```{r}
#| label: fig-log-logit-lik
#| fig-cap: "二维情形下的逻辑回归模型的负对数似然函数曲面"
#| fig-width: 6
#| fig-height: 5
#| fig-showtext: true
#| echo: false

# 目标函数
log_logit_lik0 <- function(beta, X0 = X0, y0 = y0) {
  p <- plogis(cbind(1, X0) %*% beta)
  -sum(y0 * log(p) + (1 - y0) * log(1 - p))
}

set.seed(2023)
n <- 2500
k0 <- 1
X0 <- matrix(rnorm(n * k0), ncol = k0)
y0 <- rbinom(n, size = 1, prob = plogis(1 + 3 * X0[, 1]))

alpha <- seq(0, 2, length.out = 20)
beta <- seq(2, 4, length.out = 20)

df <- expand.grid(x = alpha, y = beta)
df$fnxy <- apply(X = df, MARGIN = 1, FUN = log_logit_lik0, X0 = X0, y0 = y0)
library(lattice)
wireframe(
  data = df, fnxy ~ x * y,
  shade = TRUE, drape = FALSE,
  xlab = expression(alpha),
  ylab = expression(beta),
  zlab = list(expression(-loglik(alpha, beta)), rot = 90),
  scales = list(arrows = FALSE, col = "black"),
  screen = list(z = -30, x = -70, y = 0)
)
```

当用 Base R 函数 `optim()` 来求解时，发现 Nelder-Mead 算法收敛慢，易陷入局部最优解，即使迭代 10000 次，与真值仍然相去甚远。当用 SANN （模拟退火算法）求解此 11 维非线性无约束优化问题时，迭代 10000 次后，比较接近真值。

```{r}
optim(
  par = rep(1, 11),   # 初始值
  fn = log_logit_lik, # 目标函数
  method = "SANN",
  control = list(maxit = 10000)
)
```

根据目标函数计算其梯度，有了梯度信息，可以使用迭代效率更高的 L-BFGS-B 算法。

```{r}
# 梯度函数
log_logit_lik_grad <- function(beta) {
  p <- plogis(cbind(1, X) %*% beta)
  - t((y / p - (1 - y) / (1 - p)) * p * (1 - p))  %*% cbind(1, X) 
}

optim(
  par = rep(1, 11),   # 初始值
  fn = log_logit_lik, # 目标函数
  gr = log_logit_lik_grad, # 目标函数的梯度
  method = "L-BFGS-B"
)
```

相比于函数 `optim()`，R 包 **nloptr** 不但可以提供类似的数值优化功能，而且可以处理各类非线性约束，能力更强。仍然基于上面的优化问题， 调用 **nloptr** 包求解的代码如下：

```{r}
library(nloptr)
nlp <- nloptr(
  x0 = rep(1, 11),
  eval_f = log_logit_lik,
  eval_grad_f = log_logit_lik_grad,
  opts = list(
    "algorithm" = "NLOPT_LD_LBFGS",
    "xtol_rel" = 1.0e-8
  )
)
nlp
```

如果对数似然函数是多模态的，一般的求解器容易陷入局部最优解，推荐用 **nloptr** 包的[全局优化求解器](https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#global-optimization)。

```{r}
#| eval: false
#| echo: false
# 加载 ROI 时不要自动加载插件
Sys.setenv(ROI_LOAD_PLUGINS = FALSE)
library(ROI)
library(ROI.plugin.nloptr)
op <- OP(
  objective = F_objective(F = log_logit_lik, n = 11L),
  types = rep("C", 11), maximum = FALSE,
  bounds = V_bound(ld = -3, ud = 3, nobj = 11L)
)
nlp <- ROI_solve(op, solver = "nloptr.directL")
nlp$solution
# L-BFGS
op <- OP(
  objective = F_objective(F = log_logit_lik, n = 11L, G = log_logit_lik_grad),
  bounds = V_bound(ld = -3, ud = 3, nobj = 11L)
)
nlp <- ROI_solve(op, solver = "nloptr.lbfgs", start = rep(1,11))
nlp
```

### `glm()` {#sec-logit-glm}

Base R 提供的函数 `glm()` 拟合模型，指定联系函数为 logit 变换。

```{r}
fit_r <- glm(y ~ X, family = binomial(link = "logit"))
summary(fit_r)
```

或者也可以用函数 `glm.fit()`，效果类似，使用方式不同罢了。

```{r}
fit_r2 <- glm.fit(x = cbind(1, X), y = y, family = binomial(link = "logit"))
coef(fit_r2)
```

函数 `glm()` 的参数是一个公式，函数 `glm.fit()` 的参数是矩阵、向量，用函数 `glm()` 拟合模型，其内部调用的就是函数 `glm.fit()`。


### glmnet {#sec-logit-glmnet}

调用 **glmnet** 包的函数 `glmnet()` 拟合模型，指定指数族的具体形式为二项分布，伯努利分布是二项分布的特殊形式，也叫两点分布或0-1分布。

```{r}
#| message: false

library(Matrix)
library(glmnet)
fit_glm <- glmnet(x = X, y = y, family = "binomial")
```

逻辑回归模型系数在 L1 正则下的迭代路径图

```{r}
#| label: fig-logit-glmnet
#| fig-cap: "回归系数的迭代路径"
#| fig-width: 5
#| fig-height: 5
#| fig-showtext: true

plot(fit_glm, ylab = "回归系数")
```

从图可见，剩余两个系数是非零的，一个是 3， 一个是 -2，其余都被压缩，而接近为 0 了。

```{r}
#| label: fig-logit-glmnet-lambda
#| fig-cap: "惩罚系数的迭代路径"
#| fig-width: 5
#| fig-height: 5
#| fig-showtext: true

plot(fit_glm$lambda, ylab = expression(lambda), xlab = "迭代次数",
     main = "惩罚系数的迭代路径")
```

随着迭代的进行，惩罚系数 $\lambda$ 越来越小，接近于 0，这也是符合预期的，因为模型本来就是简单的逻辑回归，不带惩罚项。选择一个迭代趋于稳定时的 $\lambda$ 比如 0.0005247159，此时各个参数的取值如下：

```{r}
coef(fit_glm, s = 0.0005247159)
```

截距 (Intercept) 对应 $\alpha = 0.997741857$，而 $\beta_1 = 3.076358149$ 对应 V1，$\beta_2 = -1.984018387$ 对应 V2，以此类推。

## 贝叶斯计算框架 Stan {#sec-bayesian-computation-stan}

<!--
逻辑回归写得很好
[Getting the most out of logistic regression](https://gongcastro.github.io/blog/logistic-regression/logistic-regression.html)
先验分布与惩罚函数
https://github.com/jgabry/bayes-workflow-book/blob/master/bayesian-estimation.Rmd
Stan 入门写得很好 [Getting Started with Stan](https://github.com/LuZhangstat/Getting-Started-with-Stan)
-->


[Stan](https://github.com/stan-dev/stan) 是一款贝叶斯计算软件，定义了一套概率编程语言，提供 R、Python、Matlab 语言等众多的编程接口，[CmdStan](https://github.com/stan-dev/cmdstan) 是其命令行编程接口，与 Stan 版本保持同步，[**CmdStanR**](https://github.com/stan-dev/cmdstanr) 包集成 CmdStan 软件，可以非常方便地分析运行结果。下面以逻辑回归模型为例，介绍 CmdStan 框架的使用。

### 先验分布与惩罚函数 {#sec-priori-penalty}

在极大似然估计的框架下，以线性模型为例，贝叶斯估计和惩罚极大似然估计是等价的，先验分布的形式对应惩罚函数的形式，如正态先验对应于 L2 惩罚、拉普拉斯先验对应 L1 惩罚。


### 安装配置 CmdStanR {#sec-setup-cmdstanr}

CmdStan 是 Stan 的命令行接口，拥有完整的 Stan 功能，版本更新与上游的 Stan 保持同步，**CmdStanR** 是 CmdStan 的 R 语言接口 [**cmdstanr**](https://github.com/stan-dev/cmdstanr) 包，相比于 **rstan** 包更加轻量，可以更快地将 CmdStan 的新功能融入进来，**cmdstanr** 和 CmdStan 是分离的，方便用户滚动升级和版本管理。下面从 GitHub 下载最新版的[源码包](https://github.com/stan-dev/cmdstan/releases/latest)，然后编译二进制版本。

```bash
# 准备 CmdStan 存放目录
sudo mkdir -p /opt/cmdstan-2.31.1
sudo chown -R $(whoami):staff /opt/cmdstan-2.31.1
# 将压缩文件解压到
tar -xzf cmdstan-2.31.1.tar.gz -C /opt/
# 进入 CmdStan 主目录
cd cmdstan-2.31.1
# 编译出可执行的二进制文件
make build
```

设置环境变量 CMDSTAN 指向 CmdStan 安装路径，这样，加载 **cmdstanr** 包会自动使用相应版本的 CmdStan 库。设置环境变量 `CMDSTANR_NO_VER_CHECK=TRUE`，让 **cmdstanr** 包加载时不要检查 CmdStan 是不是最新版。截止写作时间，本文使用的 CmdStan 版本为 2.31.1。

```r
Sys.setenv(CMDSTAN="/opt/cmdstan-2.31.1")
Sys.setenv(CMDSTANR_NO_VER_CHECK=TRUE)
```

**cmdstanr** 包当前还在 Github 上开发，没有正式发布在 CRAN 上，安装方式如下：

```{r}
#| eval: false
#| echo: true

install.packages(
  pkgs = c("cmdstanr", "rstan", "StanHeaders"),
  repos = c("https://mc-stan.org/r-packages/", getOption("repos")),
  dependencies = TRUE
)
```


### 拟合逻辑回归模型 {#sec-logit-cmdstanr}

下面用 Stan 编码逻辑回归模型，模型代码如下：

```{verbatim, file="code/bernoulli_logit_glm.stan", lang="stan"}
```

Stan 代码主要分三部分：

1. 数据部分：

1. 参数部分：

1. 模型部分：

CmdStan 已经安装了，接下来，用 **cmdstanr** 包调 CmdStan 拟合模型，有三步：其一根据上面 Stan 代码中数据块的定义准备输入数据，其二编译上面的 Stan 代码获得可执行的模型文件，其三将数据输送到可执行的模型文件进行抽样。

```{r}
#| label: compile-logit-model
#| message: false
#| results: hide

library(cmdstanr)
# 准备数据
mdata <- list(k = k, n = n, y = y, X = X)
# 来自 stan-dev/cmdstanr
mod_logit <- cmdstan_model(
  stan_file = "code/bernoulli_logit_glm.stan",
  compile = TRUE,
  cpp_options = list(stan_threads = TRUE)
)
fit_logit <- mod_logit$sample(
  data = mdata, 
  chains = 4, 
  parallel_chains = 1,
  iter_warmup = 1000, # 每条链预处理迭代次数
  iter_sampling = 2000, # 每条链总迭代次数
  threads_per_chain = 1, # 每条链设置一个线程
  seed = 20232023,
  show_messages = FALSE, 
  refresh = 0
)
```

模型拟合结果存储为一个 [**R6**](https://github.com/r-lib/R6) 类型的数据对象，调用 summary 方法可以获得任意参数的结果，见 @tbl-logit-output 。

```{r}
#| label: tbl-logit-output
#| tbl-cap: "模型参数的贝叶斯估计结果"
#| echo: false

fit_logit$summary(c("alpha", "beta", "lp__")) |> 
  knitr::kable(digits = 3)
```

`lp__` 是后验概率密度函数取对数的值，是负值，而在前面频率派方法中，对数似然函数取了负数，所以是正的。


贝叶斯方法估计模型参数是通过抽样获得的，抽样的过程是参数迭代的过程，如果迭代过程是收敛的，估计值最终会收敛到参数的真值。一般来说，我们希望迭代的效率越高越好，只需要少量的迭代次数和短暂的时间就可以获得很好的效果。接下来，看看采样过程的诊断结果：

```{r}
fit_logit$cmdstan_diagnose()
```

每个参数都有迭代轨迹，诊断链条的平稳性很重要，有几个指标很关键：

1. 有效样本数 （Effective Sample Size，简称 ESS） $n_{eff}$ 

1. 潜在尺度缩减因子 （Potential Scale Reduction Factor）$\hat{R}$ 

采样效率可以理解为每秒迭代产生有效样本数。

```{r}
# 简化版本
fit_logit$diagnostic_summary()
```

数据是根据给定模型生成的，数据与模型吻合得很好，迭代过程没有任何发散的情况。还可以可视化的方式检查参数的迭代情况，**bayesplot** 包[@Gabry2019]提供大量的可视化函数检查迭代过程和结果，逻辑回归模型的参数 $\beta_1$ 和 $\beta_2$ 的轨迹图如下：

```{r}
#| label: fig-post-logit-trace
#| fig-cap: "参数的迭代轨迹"
#| fig-showtext: true
#| fig-width: 5
#| fig-height: 6
#| message: false

library(ggplot2)
library(bayesplot)
mcmc_trace(fit_logit$draws(c("beta[1]", "beta[2]")),
  facet_args = list(
    labeller = ggplot2::label_parsed,
    strip.position = "top",
    ncol = 1
  )
) + theme_classic()
```

马尔科夫链蒙特卡罗采样的后验分布图如下：

```{r}
#| label: fig-post-logit-hist
#| fig-cap: "参数的后验分布"
#| fig-showtext: true
#| fig-width: 5
#| fig-height: 6
#| message: false

mcmc_hist(fit_logit$draws(c("beta[1]", "beta[2]")),
  facet_args = list(
    labeller = ggplot2::label_parsed,
    strip.position = "top",
    ncol = 1
  )
) + theme_classic()
```

图检验的其他方法还有轨迹图 `mcmc_trace()`，散点图 `mcmc_scatter()`，密度图 `mcmc_dens()`，直方图 `mcmc_hist()`，自相关图 `mcmc_acf()`，区间图 `mcmc_intervals()` 和岭线图 `mcmc_areas_ridges()` 等。


**CmdStanR** 的 sample 方法默认的采样器为 NUTS 的 HMC 算法，**CmdStanR** 的封装导致不能修改采样器，读者若想修改，Stan 的命令行接口 CmdStan 支持修改采样器，不过推荐读者使用默认的 NUTS 采样器，它是根据动态调整来决定 optimal integration time。NUTS 采样器的参数 `max_depth = 10` 是可以调整的，采样的树深度。

除了 sample 方法还有优化 optimize （L-BFGS 算法）和变分推断 variational 两个优化器。

```{r}
#| results: false
# L-BFGS 算法拟合模型
fit_optim_logit <- mod_logit$optimize(
  data = mdata, # 观测数据
  init = 0, # 所有参数初值设为 0 
  refresh = 0, # 不显示迭代进程
  algorithm = "lbfgs", # 优化器
  threads = 1, # 单线程
  seed = 20232023 # 随机数种子
)
```

模型输出结果如下：

```{r}
fit_optim_logit$summary(c("alpha", "beta", "lp__"))
```

Stan 实现的 variational 方法调用自动微分变分近似推断优化器（ADVI 算法）获取参数的后验分布。

```{r}
#| results: false
# 自动微分变分近似推断
fit_advi_logit <- mod_logit$variational(
  data = mdata, # 观测数据
  init = 0, # 所有参数初值设为 0 
  refresh = 0, # 不显示迭代进程
  algorithm = "meanfield", # 优化器
  threads = 1, # 单线程
  seed = 20232023 # 随机数种子
)
```

模型输出结果见下 @tbl-logit-advi 。

```{r}
#| label: tbl-logit-advi
#| tbl-cap: "模型参数的 ADVI 结果"
#| echo: false

fit_advi_logit$summary(c("alpha", "beta", "lp__")) |> 
  knitr::kable(digits = 3)
```

::: {.callout-tip}
**CmdStanR** 始终与 CmdStan 保持同步更新，及时地用上集成到 Stan 当中的学术研究成果。而 **RStan** 及其衍生包 **rstanarm** 和 **brms** 等依赖太重，安装、更新都比较麻烦。入门 **CmdStanR** 后，可以快速转入对 Stan 底层原理的学习，有利于编码符合实际需要的复杂模型，有利于掌握常用的炼丹技巧，提高科研和工作的效率。
:::


### 拟合泊松回归模型 {#sec-poisson-cmdstanr}

先介绍泊松广义线性模型，包括模拟和计算，并和 Stan 实现的结果比较。

泊松广义线性模型如下：

$$
\begin{aligned}
\log(\lambda) &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 \\
Y &\sim \mathrm{Poisson}(\lambda)
\end{aligned}
$$

设定参数向量 $\beta = (\beta_0, \beta_1, \beta_2) = (0.5, 0.3, 0.2)$，观测变量 $X_1$ 和 $X_2$ 的均值都为0，协方差矩阵 $\Sigma$ 为

$$
\left[
 \begin{matrix}
   1.0 & 0.8  \\
   0.8 & 1.0 
 \end{matrix}
\right]
$$

模拟观测到的响应变量值和协变量值，添加漂移项

```{r}
set.seed(2023)
n <- 2500 # 样本量
beta <- c(0.5, 0.3, 0.2)
X <- MASS::mvrnorm(n, mu = rep(0, 2), Sigma = matrix(c(1, 0.8, 0.8, 1), 2))
lambda <- exp(cbind(1, X) %*% beta)
y <- rpois(n, lambda = lambda)
# 拟合泊松回归模型
fit_poisson_glm <- glm(y ~ X, family = poisson(link = "log"))
summary(fit_poisson_glm)
```

```{r}
# 对数似然函数值
log_poission_lik <- logLik(fit_poisson_glm)
# 计算 AIC AIC(fit_poisson_glm)
-2 * c(log_poission_lik) + 2 * attr(log_poission_lik, "df")
```

::: {.callout-tip}
下面用 brms 包的函数 `brm()` 拟合泊松广义线性模型，结果和 Base R 内置的 函数 `glm()` 拟合结果差不多，因编译和抽样的过程比较花费时间，速度不及 Base R。

```r
fit_poisson_brm <- brms::brm(y ~ X, family = poisson())
loo(fit_poisson_brm)
```
:::

以上是简单的泊松型广义线性模型，下面增加一点复杂度，模拟泊松型随机过程模型。




### 预测核污染的分布 {#sec-nuclear-pollution-concentration}

本小节将基于真实数据的分析和建模，任务是预测核污染浓度在朗格拉普岛上的分布。

[朗格拉普环礁（Rongelap Atoll）](https://en.wikipedia.org/wiki/Rongelap_Atoll)是太平洋上的一座包含 61 个大小岛屿的环礁，它是马绍尔群岛的一部分，二战后，其中的朗格拉普岛成为美国在太平洋的一个领地，1954 年，美国在该岛进行了氢弹核武器测试，产生大量的核污染，严重破坏了当地的生态。几十年后，一批科学家登陆该岛研究生态恢复情况，评估当地居民重返家园的可行性，收集了一些数据。数据集 `rongelap` 记录了 157 个测量点的伽马射线强度，即在时间间隔 `time` 内放射的粒子数目 `counts`，测量站点的横纵坐标分别为 `cX` 和 `cY`。下 @tbl-rongelap-nuclear-data 展示部分朗格拉普岛核污染检测数据及海岸线坐标数据。

```{r}
#| label: tbl-rongelap-nuclear-data
#| tbl-cap: "朗格拉普岛核污染检测数据及海岸线坐标数据"
#| tbl-subcap: 
#|   - "核污染检测数据"
#|   - "海岸线坐标数据"
#| layout-ncol: 2
#| echo: false

rongelap <- readRDS(file = "data/rongelap.rds")
rongelap_coastline <- readRDS(file = "data/rongelap_coastline.rds")

library(knitr)
knitr::kable(head(rongelap, 6),
  col.names = c("横坐标", "纵坐标", "数目", "时间")
)
knitr::kable(head(rongelap_coastline, 6),
  col.names = c("横坐标", "纵坐标")
)
```

::: {.callout-note}
之前，[Ole F. Christensen](http://gbi.agrsci.dk/~ofch/) 和 Paulo J. Ribeiro Jr 将 rongelap 数据集存放在 **geoRglm** 包内，后来，**geoRglm** 不维护，已从 CRAN 移除了。
:::

朗格拉普岛呈月牙形，采样点在岛上的分布如 @fig-rongelap-location 所示，在岛屿的东北和西南方各有两个密集采样区。

```{r}
#| label: fig-rongelap-location
#| fig-cap: "采样点在岛上的分布"
#| echo: false
#| fig-showtext: true
#| fig-width: 7
#| fig-height: 4

library(lattice)

xyplot(cY ~ cX,
  data = rongelap, pch = 19, cex = 0.25,
  xlab = "横坐标",
  ylab = "纵坐标",
  scales = list(    
    # 去掉图形上边、右边多余的刻度线
    x = list(alternating = 1, tck = c(1, 0)),
    y = list(alternating = 1, tck = c(1, 0))
    ),
  # 减少三维图形的边空
  lattice.options = list(
    layout.widths = list(
      left.padding = list(x = 0, units = "inches"),
      right.padding = list(x = -0.15, units = "inches")
    ),
    layout.heights = list(
      bottom.padding = list(x = -0.1, units = "inches"),
      top.padding = list(x = -0.2, units = "inches")
    )
  ),
  panel = function(...) {
    panel.grid(h = -1, v = -1, ...)
    panel.points(col = "black", ...)
    panel.lines(x = rongelap_coastline$cX, 
                y = rongelap_coastline$cY, 
                col = "black")
  },
  par.settings = list(axis.line = list(col = "black"))
)
```

朗格拉普岛上各个检测站点的核污染浓度，如 @fig-rongelap-location-zoom 所示，鉴于四个集中检测区密集的采样阵列，通过局部放大，突出展示最左侧的一个检测区。

```{r}
#| label: fig-rongelap-location-zoom
#| fig-cap: "岛上各采样点的核辐射强度"
#| fig-width: 8
#| fig-height: 4
#| fig-showtext: true
#| echo: false

library(ggplot2)

p1 <- ggplot() +
  geom_path(data = rongelap_coastline, aes(x = cX, y = cY)) +
  geom_point(data = rongelap, aes(x = cX, y = cY, color = counts / time), size = 0.5) +
  scale_x_continuous(n.breaks = 7) +
  scale_color_binned(type = colorspace::scale_color_binned_sequential, palette = "Grays") +
  geom_segment(
    data = data.frame(x = -5560, xend = -5000, y = -3000, yend = -2000),
    aes(x = x, y = y, xend = xend, yend = yend),
    arrow = arrow(length = unit(0.03, "npc"))
  ) +
  theme_bw() +
  labs(x = "横坐标", y = "纵坐标", color = "放射强度")

p2 <- ggplot() +
  geom_point(data = rongelap, aes(x = cX, y = cY, color = counts / time), size = 2, show.legend = FALSE) +
  scale_color_binned(type = colorspace::scale_color_binned_sequential, palette = "Grays") +
  coord_cartesian(xlim = c(-5700, -5540), ylim = c(-3260, -3100)) +
  theme_bw() +
  labs(x = NULL, y = NULL)

p1
print(p2, vp = grid::viewport(x = .25, y = .725, width = .275, height = .45))
```  

如 @fig-rongelap-concentration 所示，在朗格拉普岛上采样点的位置及检测到的辐射强度，射线越高表示放射性越强。这是用 **lattice** 包的 `cloud()` 函数绘制的，以点和线构成射线形象地表达伽马射线的放射强度。

```{r}
#| label: fig-rongelap-concentration
#| fig-cap: "岛上各采样点的放射强度"
#| echo: false
#| fig-showtext: true
#| fig-width: 6
#| fig-height: 5

cloud(counts / time ~ cX * cY,
  data = rongelap, col = "black",
  scales = list(arrows = FALSE, col = "black"),
  xlab = list("横坐标", rot = 20), 
  ylab = list("纵坐标", rot = -50),  
  zlab = list("放射强度", rot = 90),  
  type = c("p", "h"), pch = 16, lwd = 0.5,
  # 设置三维图的观察方位
  screen = list(z = 30, x = -65, y = 0)
)
```

核污染是由辐射元素衰变产生的，通常用单位时间释放出来的粒子数目表示辐射强度，也是核污染浓度，因此，建立泊松型广义线性模型分析数据。

```{r}
fit_rongelap_glm <- glm(counts ~ cX + cY,
  family = poisson(link = "log"),
  offset = log(time), data = rongelap
)
summary(fit_rongelap_glm)
```

::: {.callout-note}
当 `family = poisson(link = "log")` 时，响应变量只能放正整数。参数 `offset` 的作用介绍见统计之都[论坛](https://d.cosx.org/d/420881)，放射性粒子的数量 `counts` 和检测时间长度有关。
:::

从这个简单的广义线性模型结果，不难看出，位置变量 `cX` 和 `cY` 是显著的，从实际场景出发，也不难理解，位置信息是非常关键的。进一步，充分利用位置信息，精细建模是很有必要的。相邻位置的核污染浓度是相关的，离得近的比离得远的更相关。简单的广义线性模型并没有考虑距离相关性，它认为各个观测点的数据是相互独立的。因此，考虑采用广义线性混合效应模型，在广义线性模型的基础上添加位置相关的随机效应，用以刻画未能直接观测到的潜在影响。简单起见，假定随机效应之间是相互独立的高斯分布。

$$
\begin{aligned}
\log(t_{j}\lambda_{j}) &= \beta_{0j} \\
\beta_{0j} &=\gamma_{00} + U_{0j}
\end{aligned}
$$

其中响应变量服从泊松分布，即 $Y_{j} \sim \mathrm{Poisson}(t_{j}\lambda_{j})$ ，随机效应 $\beta_{0j}$ 服从均值为 0，方差为 $\tau_{00}^2)$ 的高斯分布，即 $U_{0j} \sim \mathcal{N}(0,\tau_{00}^2)$ ，157 个检测站点的残差服从独立同分布的高斯分布，即 $\epsilon_{j} \sim \mathcal{N}(0,\sigma^2)$，$j=1,2,\cdots,157$ 。


::: {.callout-tip}
根据目前已有的数据，没有收集影响放射强度的其它因素，也就没有其它观测变量，所以，只有截距项是变化的。如果有其他因素，可以在 $\beta_{0j}$ 后加入，比如 $\beta_{1j}x_{j}$。进一步，如果在 157 个位置处反复测量多次放射数据，则模型又可变为重复测量模型。

$$
\begin{aligned}
\log(t_{ij}\lambda_{ij}) &= \beta_{0j} +\beta_{1j}x_{1ij} \\
\beta_{0j} &=\gamma_{00} + U_{0j} \\
\beta_{1j} &=\gamma_{10} + U_{1j} \\
\begin{bmatrix}
    U_{0j} \\
    U_{1j}
\end{bmatrix}
&\sim \mathcal{N}
\left(
\begin{bmatrix}
    0  \\
    0 
\end{bmatrix}
,
\begin{bmatrix}
    \tau_{00}^2 & \tau_{01} \\
    \tau_{01} & \tau_{10}^2
\end{bmatrix}
\right)
\end{aligned}
$$

其中下标 $i$ 代表重复测量，则 $Y_{ij}$ 表示第 $i$ 次在第 $j$ 个位置检测到的放射量，$x_{1ij}$ 表示第1个协变量第$i$次在第$j$个位置上的观测值，一种非常贴合实际的情形是 $x_{1ij}$ 就代表时间 $T_{ij}$ ，表示科学家每隔 5 年登岛收集数据，此后登岛多次，$T_{ij}$ 意味着第 $i$ 次登岛收集各个检测点的放射数据。变截距 $\beta_{0j}$ 和变系数 $\beta_{1j}$ 同时存在，且二者联合分布为二元正态分布，残差仍然假定独立同高斯分布 $\epsilon_{ij} \sim \mathcal{N}(0,\sigma^2)$ 。
:::

```{r}
library(lme4)
rongelap$dummary <- 1:157
fit_rongelap_glmer <- glmer(counts ~ 1 + (1 | dummary),
  family = poisson(link = "log"),
  offset = log(time), data = rongelap
)
summary(fit_rongelap_glmer)
```

从拟合模型的结果来看，参数 $\gamma_{00} = 1.94405, \tau_{00}^2 = 0.2223$ 。将每个位置看作单独的一个随机效应，比较两个模型的对数似然，后者变大了，这意味着模型拟合效果更好了。

```{r}
# 广义线性模型
logLik(fit_rongelap_glm)
# 广义线性混合效应模型
logLik(fit_rongelap_glmer)
```

进一步，去掉随机效应相互独立的假设，随机效应之间存在相关性结构，这更符合位置效应存在相互影响的实际情况，也因此要引入带空间随机效应的广义线性混合效应模型。空间广义线性混合效应模型在统计中具有重要的地位，在地质统计、卫生统计、气象统计和空间计量等领域有广泛的应用，如分析地区范围内的疟疾分布，有限气象站点条件下，预测地区 PM2.5 污染物浓度分布等。

1998 年 Peter J. Diggle 等提出蒙特卡罗极大似然方法估计不带块金效应的响应变量服从泊松分布的空间广义混合效应模型的参数，分析了朗格拉普岛上核污染浓度的空间分布[@Diggle1998]。2004 年 Ole F Christensen 在 Peter J. Diggle 等人的基础上添加了块金效应，同样使用蒙特卡罗极大似然方法估计了模型中的参数[@Christensen2004]。

$$
\begin{aligned}
\log\{\lambda(x_i)\} & =  \beta + S(x_{i}) \\
\log\{\lambda(x_i)\} & =  \beta + S(x_{i}) + Z_{i}
\end{aligned}
$$

根据 ${}^{137}\mathrm{Cs}$ 放出的伽马射线在 $N=157$ 站点不同时间间隔 $t(x_{i})$ 的放射量，建立泊松广义线性混合效应模型。模型中，截距 $\beta$ 相当于平均水平，放射粒子数作为响应变量服从均值为 $t(x_{i})\lambda(x_i)$ 的泊松分布，即 $Y_{i} \sim \mathrm{Poisson}(t(x_{i})\lambda(x_i))$，$\lambda(x_i)$ 可理解为辐射强度，$S(x_{i})$ 表示位置 $x_i$ 处的空间效应，服从指数型自协方差函数为 

$$
\mathrm{Cov}( S(x_i), S(x_j) ) = \sigma^2 \exp( -\|x_i -x_j\|_{2} / \phi )
$$

的平稳空间高斯过程 $S(x),x \in \mathbb{R}^2$，且 $Z_i$ 之间相互独立同正态分布 $\mathcal{N}(0,\tau^2)$ ，$Z_i$ 表示与空间效应无关的块金效应，即非空间的随机效应，可以理解为测量误差或空间变差，这里 $i = 1,\ldots, 157$。

[**glmmTMB**](https://github.com/glmmTMB/glmmTMB) 包可以拟合空间广义线性混合效应模型，使用语法和 **lme4** 包一样。

```{r}
# 位置标准化
rongelap$loc <- glmmTMB::numFactor(scale(rongelap$cX), scale(rongelap$cY))
# 位置 ID
rongelap$ID <- factor(rep(1, nrow(rongelap)))
# 拟合模型
fit_rongelap_sglmer <- glmmTMB::glmmTMB(
  formula = counts ~ 1 + mat(loc + 0 | ID),
  data = rongelap, family = poisson(link = "log"),
  offset = log(time)
)
```

其中协方差结构由核函数决定，这里采用广泛使用的 Matérn 型，**glmmTMB** 包 还支持指数型`exp` 和高斯型 `gau`，协方差矩阵是 $157 \times 157$ 维的。对数似然函数的值又变大了，拟合效果更好了，相比于从广义线性模型到广义线性混合效应模型，提升不是特别多，一般来说，模型越复杂提升空间也会越小，过于复杂也没有必要。

```{r}
logLik(fit_rongelap_sglmer)
```

`summary(fit_rongelap_sglmer)` 输出中包含大型的协方差矩阵，协方差矩阵也可单独抽取见 `ranef(fit_rongelap_sglmer)`，篇幅所限，此处略去。输出结果中，模型的固定效应 `fixef(fit_rongelap_sglmer)` （此处也是截距）为 1.82007，随机效应的方差为 0.2929。


## 机器学习框架 tensorflow {#sec-machine-learning-tensorflow}

### 软件配置 {#sec-setup-tensorflow}

推荐读者配置独立的 Python 虚拟环境，在虚拟环境中来做算法开发，下面用 virtualenv 配置一个开发环境。

如果读者是 MacOS 系统，推荐安装命令行工具 Command Line Tools for Xcode，它提供大量常用的开发工具，如 Apple LLVM compiler, linker, and Make，Git，Python 等。安装过程非常简单，只需如下一行命令。

```bash
xcode-select --install
```

也可以在网上先下载[工具包](https://developer.apple.com/download/all/)，再离线安装。最近的 MacOS 系统已经自带了 Python 3.9.x，所以，只需在系统中安装 [virtualenv](https://github.com/pypa/virtualenv) 软件。推荐使用软件包管理工具[brew](https://github.com/Homebrew/brew)来安装。

```bash
brew install virtualenv
```

用 virtualenv 创建虚拟环境，虚拟环境的存放路径是 `/opt/.virtualenvs/r-tensorflow`，所以名字就是 `r-tensorflow`

```bash
# 准备虚拟环境萼存放地址
sudo mkdir -p /opt/.virtualenvs/r-tensorflow
# 赋予当前用户的读写权限
sudo chown -R $(whoami):staff /opt/.virtualenvs/r-tensorflow
# 方便后续复用
export RETICULATE_PYTHON_ENV=/opt/.virtualenvs/r-tensorflow
# 创建虚拟环境
virtualenv -p /usr/bin/python3 $RETICULATE_PYTHON_ENV
# 激活虚拟环境
source $RETICULATE_PYTHON_ENV/bin/activate
```

激活虚拟环境后，进入书籍仓库根目录，从指定文件 requirements.txt 安装 Python 模块。

```bash
pip install -r requirements.txt
```

安装完成后，可以在命令行中输入 `deactivate` 退出虚拟环境。至此，所需的 Python 开发环境已经准备好了。

接下来配置 R 环境，使得可以在 R 语言中调用 tensorflow 模块，搭建机器学习模型。在文件 `.Rprofile` 里设置环境变量 `RETICULATE_PYTHON` 和 `RETICULATE_PYTHON_ENV`，这样 [**reticulate**](https://github.com/rstudio/reticulate) 包就能发现和使用它了。

```bash
Sys.setenv(RETICULATE_PYTHON="/opt/.virtualenvs/r-tensorflow/bin/python")
Sys.setenv(RETICULATE_PYTHON_ENV="/opt/.virtualenvs/r-tensorflow")
```

最后，输入命令 `reticulate::py_config()` 检查配置情况，看到如下结果，表示配置成功。

```
python:         /opt/.virtualenvs/r-tensorflow/bin/python
libpython:      /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/config-3.9-darwin/libpython3.9.dylib
pythonhome:     /opt/.virtualenvs/r-tensorflow:/opt/.virtualenvs/r-tensorflow
virtualenv:     /opt/.virtualenvs/r-tensorflow/bin/activate_this.py
version:        3.9.6 (default, Oct 18 2022, 12:41:40)  [Clang 14.0.0 (clang-1400.0.29.202)]
numpy:          /opt/.virtualenvs/r-tensorflow/lib/python3.9/site-packages/numpy
numpy_version:  1.24.2

NOTE: Python version was forced by RETICULATE_PYTHON
```


::: {.callout-tip}
如果希望打开终端就进入虚拟环境，可以在 `~/.zshrc` 文件中添加两行：

```bash
export RETICULATE_PYTHON_ENV=/opt/.virtualenvs/r-tensorflow
source $RETICULATE_PYTHON_ENV/bin/activate
```
:::
